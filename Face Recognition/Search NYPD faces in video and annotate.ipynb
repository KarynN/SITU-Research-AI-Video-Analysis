{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8830,"status":"ok","timestamp":1725376601874,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"OYMIN81tlKwT","outputId":"51dd833e-8304-4389-868c-6abff78590dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":43894,"status":"ok","timestamp":1725376645762,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"aGit-xTFdkz3","outputId":"5ef30c02-ca0f-469b-c50e-94806f0d3941","cellView":"form"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mCloning into 'Arc2Face'...\n","remote: Enumerating objects: 126, done.\u001b[K\n","remote: Counting objects: 100% (126/126), done.\u001b[K\n","remote: Compressing objects: 100% (103/103), done.\u001b[K\n","remote: Total 126 (delta 57), reused 67 (delta 20), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (126/126), 29.07 MiB | 29.74 MiB/s, done.\n","Resolving deltas: 100% (57/57), done.\n"]}],"source":["#@title **INSTALLS**\n","!pip install -q deepface\n","!pip install -q face_recognition\n","!pip install -q mediapipe\n","!pip install -q opencv-python\n","!git clone https://github.com/foivospar/Arc2Face.git\n","#!pip install -r /content/Arc2Face/requirements.txt\n","!wget -q -O detector.tflite -q https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":173604,"status":"ok","timestamp":1725376819351,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"5CRZq0ovFwS-","outputId":"60a94d4c-0bb4-40ba-85af-677d63da36c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: line 1: 1.24.0: No such file or directory\n","Collecting torch==2.0.1\n","  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.15.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.0.0 (from torch==2.0.1)\n","  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (71.0.4)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.2)\n","Collecting lit (from triton==2.0.0->torch==2.0.1)\n","  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n","Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.4.0+cu121\n","    Uninstalling torch-2.4.0+cu121:\n","      Successfully uninstalled torch-2.4.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 2.0.1 which is incompatible.\n","torchvision 0.19.0+cu121 requires torch==2.4.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n","Collecting torchvision==0.15.2\n","  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.32.3)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.0.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (3.15.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision==0.15.2) (71.0.4)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision==0.15.2) (0.44.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision==0.15.2) (3.30.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision==0.15.2) (18.1.8)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision==0.15.2) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision==0.15.2) (1.3.0)\n","Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchvision\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.19.0+cu121\n","    Uninstalling torchvision-0.19.0+cu121:\n","      Successfully uninstalled torchvision-0.19.0+cu121\n","Successfully installed torchvision-0.15.2\n","Collecting diffusers==0.23.0\n","  Downloading diffusers-0.23.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0) (8.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0) (3.15.4)\n","Requirement already satisfied: huggingface-hub>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0) (0.23.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0) (2.32.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0) (0.4.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0) (9.4.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.23.0) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.23.0) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.23.0) (6.0.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.23.0) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.23.0) (4.12.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.23.0) (3.20.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.23.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.23.0) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.23.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.23.0) (2024.7.4)\n","Downloading diffusers-0.23.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: diffusers\n","Successfully installed diffusers-0.23.0\n","Collecting peft\n","  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.0.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.42.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.32.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.4)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (71.0.4)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (0.44.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.30.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (18.1.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: peft\n","Successfully installed peft-0.12.0\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (71.0.4)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.44.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.30.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (18.1.8)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2024.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Collecting insightface\n","  Downloading insightface-0.7.3.tar.gz (439 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from insightface) (1.26.4)\n","Collecting onnx (from insightface)\n","  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from insightface) (4.66.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from insightface) (2.32.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from insightface) (3.7.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from insightface) (9.4.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from insightface) (1.13.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from insightface) (1.3.2)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from insightface) (0.23.2)\n","Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from insightface) (1.13)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from insightface) (3.0.11)\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (from insightface) (1.4.14)\n","Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from insightface) (3.11.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (6.0.2)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (4.12.2)\n","Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (2.8.2)\n","Requirement already satisfied: albucore>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (0.0.13)\n","Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (0.2.0)\n","Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface) (4.10.0.84)\n","Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (3.3)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (2.34.2)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (2024.8.24)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (24.1)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface) (0.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface) (2.8.2)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->insightface) (4.25.4)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->insightface) (0.2.13)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->insightface) (2024.7.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->insightface) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->insightface) (3.5.0)\n","Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from albucore>=0.0.13->albumentations->insightface) (2.0.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations->insightface) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations->insightface) (2.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->insightface) (1.16.0)\n","Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: insightface\n","  Building wheel for insightface (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for insightface: filename=insightface-0.7.3-cp310-cp310-linux_x86_64.whl size=1055392 sha256=b63f89876b68796ef9fa8f29ea8db4d9d708a731e661626f20c07f0e3e0a8f57\n","  Stored in directory: /root/.cache/pip/wheels/e3/d0/80/e3773fb8b6d1cca87ea1d33d9b1f20a223a6493c896da249b5\n","Successfully built insightface\n","Installing collected packages: onnx, insightface\n","Successfully installed insightface-0.7.3 onnx-1.16.2\n","Collecting onnxruntime-gpu\n","  Downloading onnxruntime_gpu-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n","Collecting coloredlogs (from onnxruntime-gpu)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.3.25)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (4.25.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.13.2)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n","Downloading onnxruntime_gpu-1.19.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (223.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.1/223.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.19.0\n","Collecting gradio\n","  Downloading gradio-4.42.0-py3-none-any.whl.metadata (15 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Collecting fastapi (from gradio)\n","  Downloading fastapi-0.112.2-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.3.0 (from gradio)\n","  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting httpx>=0.24.1 (from gradio)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.5)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.4)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n","Collecting starlette<0.39.0,>=0.37.2 (from fastapi->gradio)\n","  Downloading starlette-0.38.4-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gradio-4.42.0-py3-none-any.whl (16.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Downloading ruff-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.112.2-py3-none-any.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.38.4-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n","  Attempting uninstall: tomlkit\n","    Found existing installation: tomlkit 0.13.2\n","    Uninstalling tomlkit-0.13.2:\n","      Successfully uninstalled tomlkit-0.13.2\n","Successfully installed aiofiles-23.2.1 fastapi-0.112.2 ffmpy-0.4.0 gradio-4.42.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.3 semantic-version-2.10.0 starlette-0.38.4 tomlkit-0.12.0 uvicorn-0.30.6 websockets-12.0\n","Collecting transformers==4.33.0\n","  Downloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (0.23.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (2.32.3)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (0.4.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (2024.7.4)\n","Downloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.42.4\n","    Uninstalling transformers-4.42.4:\n","      Successfully uninstalled transformers-4.42.4\n","Successfully installed tokenizers-0.13.3 transformers-4.33.0\n"]}],"source":["!pip install numpy<1.24.0\n","!pip install torch==2.0.1\n","!pip install torchvision==0.15.2\n","!pip install diffusers==0.23.0\n","!pip install peft\n","!pip install accelerate\n","!pip install insightface\n","!pip install onnxruntime-gpu\n","!pip install gradio\n","!pip install transformers==4.33.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311,"referenced_widgets":["b0dcd71240c44a20baccfc45d5a1d33f","7d5d08dcfc994e38b128edefc747d61f","53e43a5cbd4541cda769b09e5b210694","ee70a9f244d74ff5bc072189dfb5b453","2ed171c407264eef81cbfcaffefc8e97","489f7118949046ae837bb7e8d01275c4","b4235b7bd79c429a87893bb654de76c4","0206f067627d46f88d85952e43b6e17b","ed64d4fedb194070b9521dd3c6129a2f","91309492784843aa8a3a2543b8ab63bd","874ba4cc5bf643feb474bd5007fc13e1","c88f10a7cff8450fbf20ea82b4236850","61cf0f3107e3489395efb95422a05fc2","ee177a0daa57452dade440c5fa1d0ae2","13eaedaf13c240a3acd07bba5cdb23ca","4ae2d051e3584d3c850ccacd510bccb4","ae8f34d87a58406ca5220d3aed01ec29","6518a7f83c564b819738852a588e8f74","55b15109da4c4578923062a3d44527f8","2a670ac2de644862adb36beab06c4923","f30b1f8896b349af846e015d0b0ba160","29eee78998a7498c92a6872de2ffad2d","3044309fc77b40feaa12d172fb1c4f6e","64f076f2003d4155b73c5a9aa8bfae40","074ddb51f9004ea79c66394da0196a11","4ce559e8ecf042fdb317d9f6b55851fc","67aeab169b1d44dc8236af06a6533065","6dca33c465cb49509078b93b81a77520","9ead95cbd9f5452799a155c40b7e8278","2eb1511489544cc9b6cc8b037317561b","5f6c0d85509542a7a899e4f397fef37d","8151cb09dfda4a30aaf92eaa2cf6c4aa","eb4daddf094e41d68e1f460e2c59f7b9","39e31e4fe03c40569f4fa5d496eccec9","574ff0a361e245f8a8407300bd1d1382","e7b01e10ab1a48979e6f49b94f92e1eb","ede265d81a574adc84baf10be2fdc1ca","6a34160e018a480a82344a24a9cc9a2f","146b8abf6bd34493bbad696b0df3e202","6d1c5981d2894f35ac8f5d30ffe6161e","abc3537f0ef44211b7b3c236766fc499","aa0929f71d934b52b76497456b2d729e","53eefb632b7d450caa3c536ab84a2f3e","932bf349ce28475080ceea5a62ea87df","05e1d5f39d0f4a609a165b2b044fdbc7","6531368ec5da4835899d15625aae01c3","645cbe40071a4c31bb1b09971e75cbbe","eb28a9116cda478eb55389dbf289122b","83b43ac0d98840528a2599e5297c36b8","84e447f30ca54c3cab4c1df1ae4f6f08","9b9af81379344a228c64839c406e5b95","b891923404c44a43aebfd215d86e15d2","43a8eaf15df04a94872fc19fa0517608","721b131188594c388a4f55cf6c1a4fb9","9102f1be02d64b4d96e82415dc239d96"]},"executionInfo":{"elapsed":73981,"status":"ok","timestamp":1725376893253,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"VIVSh6dV9ZGm","outputId":"02967a81-04de-4737-fe86-3b7a14f9c88c","cellView":"form"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0dcd71240c44a20baccfc45d5a1d33f","version_major":2,"version_minor":0},"text/plain":["arc2face/config.json:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c88f10a7cff8450fbf20ea82b4236850","version_major":2,"version_minor":0},"text/plain":["diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3044309fc77b40feaa12d172fb1c4f6e","version_major":2,"version_minor":0},"text/plain":["encoder/config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39e31e4fe03c40569f4fa5d496eccec9","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/492M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05e1d5f39d0f4a609a165b2b044fdbc7","version_major":2,"version_minor":0},"text/plain":["arcface.onnx:   0%|          | 0.00/261M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["#@title **IMPORT REQS**\n","!mkdir -p ./models/antelopev2\n","#!mkdir -p ./search_images\n","#!mkdir -p ./search_videos\n","\n","import pickle\n","import os\n","import pandas as pd\n","import numpy as np\n","import json\n","import logging\n","from tqdm.notebook import tqdm\n","import cv2\n","import math\n","import shutil\n","from sklearn.metrics.pairwise import cosine_similarity\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","import mediapipe as mp\n","from mediapipe.tasks import python\n","from mediapipe.tasks.python import vision\n","from typing import Tuple, Union\n","\n","import sys\n","sys.path.append('/content/Arc2Face')\n","\n","import insightface\n","from insightface.app import FaceAnalysis\n","from arc2face import CLIPTextModelWrapper, project_face_embs\n","import torch\n","from insightface.app.face_analysis import Face\n","\n","from huggingface_hub import hf_hub_download\n","\n","hf_hub_download(repo_id=\"FoivosPar/Arc2Face\", filename=\"arc2face/config.json\", local_dir=\"./models\")\n","hf_hub_download(repo_id=\"FoivosPar/Arc2Face\", filename=\"arc2face/diffusion_pytorch_model.safetensors\", local_dir=\"./models\")\n","hf_hub_download(repo_id=\"FoivosPar/Arc2Face\", filename=\"encoder/config.json\", local_dir=\"./models\")\n","hf_hub_download(repo_id=\"FoivosPar/Arc2Face\", filename=\"encoder/pytorch_model.bin\", local_dir=\"./models\")\n","\n","hf_hub_download(repo_id=\"FoivosPar/Arc2Face\", filename=\"arcface.onnx\", local_dir=\"./models/antelopev2\")\n","\n","print(\"done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"uuLnGntx7SZC"},"outputs":[],"source":["#@title **SET UP DICT**\n","\n","!cp -r /content/drive/MyDrive/models/antelopev2/* /content/models/antelopev2\n","\n","\n","base_folder = \"/content/drive/MyDrive/faces\" #@param {type:\"string\"}\n","emb_file = base_folder + '/NYPD_profile_embeddings.pkl'\n","json_file = base_folder + '/NYPD_profile_dict.json'\n","\n","\n","json_df = pd.read_json(json_file)\n","json_df.reset_index()\n","\n","json_df = json_df[pd.notna(json_df['source_link'])]\n","\n","img_names = []\n","for index, row in json_df.iterrows():\n","    badge_number = row['badge_number'][1:] if pd.notna(row['badge_number']) and row['badge_number'] else 'unknown'\n","    if pd.notna(row['allegations']):\n","        allegations = int(row['allegations']) if isinstance(row['allegations'], float) else row['allegations']\n","    else:\n","        allegations = 'unknown'\n","    name = row['name'].replace(' ', '_').replace(',', '').replace('.', '') if pd.notna(row['name']) and row['name'] else 'unknown'\n","    img_name = f\"{badge_number}_{allegations}_{name}.jpg\"\n","    img_names.append(img_name)\n","\n","json_df['img_name'] = img_names\n","\n","with open(emb_file, 'rb') as f:\n","    emb_data = pickle.load(f)\n","    emb_data = list(zip(emb_data[0], emb_data[1]))\n","    emb_df = pd.DataFrame(emb_data, columns=['img_name', 'embedding'])\n","\n","\n","merged = pd.merge(json_df, emb_df, left_on='source_link', right_on='img_name')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1725376912778,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"HVdbq2hMXhlS","outputId":"13fd179d-8474-4248-a7df-ff4931e84ef0"},"outputs":[{"name":"stdout","output_type":"stream","text":["11649\n"]}],"source":["non_faces = ['#17032', '#10171', '#13404', '#16688', '#648', '#7320', '#13767', '#2126', '#5207', '#3061', '#13449', '#9322', '#19022', '#15035', '#6901', '#3001', '#4736', '#2543', '#11435', '#19708', '#12214', '#14247', '#13749', '#14629', '#7535', '#21593', '#3754', '#3667', '#86']\n","\n","filtered = merged[~merged['badge_number'].isin(non_faces)]\n","\n","print(len(filtered))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1794,"status":"ok","timestamp":1725376914565,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"rPZbAjS3j6GN","outputId":"c65f89b7-0513-4844-a5be-f862d94d1c50","cellView":"form"},"outputs":[{"name":"stdout","output_type":"stream","text":["Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n","find model: ./models/antelopev2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n","Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n","find model: ./models/antelopev2/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n","Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n","find model: ./models/antelopev2/arcface.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n","Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n","find model: ./models/antelopev2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n","Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n","find model: ./models/antelopev2/scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0\n","set det-size: (128, 128)\n"]}],"source":["#@title **EMBED FUNCTIONS**\n","\n","app = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n","app.prepare(ctx_id=0, det_size=(128, 128))\n","\n","def expand_and_pad_image(image, target_size=512):\n","    # Get the current size of the image\n","    h, w, _ = image.shape\n","\n","    # Determine the scaling factor to resize the image\n","    scale_factor = target_size / max(h, w)\n","\n","    # Resize the image\n","    new_w = int(w * scale_factor)\n","    new_h = int(h * scale_factor)\n","    resized_image = cv2.resize(image, (new_w, new_h))\n","\n","    # Calculate padding values to center the image\n","    pad_w = (target_size - new_w) // 2\n","    pad_h = (target_size - new_h) // 2\n","\n","    # Create a new image with padding\n","    padded_image = cv2.copyMakeBorder(\n","        resized_image,\n","        pad_h,\n","        target_size - new_h - pad_h,\n","        pad_w,\n","        target_size - new_w - pad_w,\n","        cv2.BORDER_CONSTANT,\n","        value=[0, 0, 0]\n","    )\n","\n","    return padded_image\n","\n","def convert_images_to_jpg(directory):\n","    for filename in os.listdir(directory):\n","        if filename.endswith(('.png', '.jpeg', '.bmp', '.gif', '.tiff')):\n","            img_path = os.path.join(directory, filename)\n","            img = Image.open(img_path)\n","            if img.mode in ('RGBA', 'LA'):\n","                img = img.convert('RGB')\n","            new_filename = os.path.splitext(filename)[0] + '.jpg'\n","            new_path = os.path.join(directory, new_filename)\n","            img.save(new_path, 'JPEG')\n","            os.remove(img_path)\n","\n","            #print(f\"Converted {filename} to {new_filename} and deleted the original file\")\n","\n","\n","def generate_embedding(img):\n","    padded_img = expand_and_pad_image(img, target_size=512) #in BGR\n","    faces = app.get(padded_img)\n","    if faces:\n","        id_emb = torch.tensor(faces[0]['embedding'], dtype=torch.float32)[None].cuda()\n","        id_emb = id_emb / torch.norm(id_emb, dim=1, keepdim=True)\n","        id_emb = id_emb.cpu()\n","        return id_emb\n","    return None\n","\n","def process_images_in_folder(folder_path):\n","    convert_images_to_jpg(folder_path)\n","    embeddings = []\n","    image_names = []\n","    all_image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n","\n","    for image_file in all_image_files:\n","      imgpath = os.path.join(folder_path, image_file)\n","      img = np.array(Image.open(imgpath))[:,:,::-1]\n","\n","      embedding = generate_embedding(img)\n","      if embedding is not None:\n","            embeddings.append(embedding.numpy())\n","            image_names.append(image_file)\n","            print(\"Embedded \" + image_file)\n","    search_data = list(zip(image_names, embeddings))\n","    search_df = pd.DataFrame(search_data, columns=['img_name', 'embedding'])\n","    return search_df\n","\n","\n","#search_df = process_images_in_folder()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqmbnS77oDCY","cellView":"form"},"outputs":[],"source":["#@title **SEARCH FUNCTION SETUP**\n","\n","def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n","\n","def find_top_k_similar_embeddings(search_embedding, df, k=10):\n","    similarities = []\n","\n","    # Convert search_embedding to the correct shape for cosine_similarity\n","    #search_embedding = np.array(search_embedding)#.reshape(1, -1)\n","\n","    for index, row in df.iterrows():\n","        print(search_embedding)\n","        other_embedding = row['embedding'][0]#.reshape(1, -1)\n","        similarity = cosine_similarity(search_embedding, other_embedding)#[0][0]\n","        similarities.append((row['name'], row['badge_number'], similarity, row['img_name']))\n","\n","    # Sort by similarity in descending order and get top k\n","    similarities.sort(key=lambda x: x[2], reverse=True)\n","    top_k_similarities = similarities[:k]\n","\n","    return top_k_similarities\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"elapsed":382,"status":"error","timestamp":1724090004915,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"eLyZ8X7_64Wo","outputId":"8989883b-86d4-49c5-9494-e41591690d57"},"outputs":[{"ename":"NameError","evalue":"name 'mp' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-06e72f4ce6c5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title **MP FACE DETECT**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mBaseOptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseOptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mFaceDetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFaceDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFaceDetectorOptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFaceDetectorOptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'mp' is not defined"]}],"source":["#@title **MP FACE DETECT**\n","\n","BaseOptions = mp.tasks.BaseOptions\n","FaceDetector = mp.tasks.vision.FaceDetector\n","FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n","VisionRunningMode = mp.tasks.vision.RunningMode\n","\n","def initialize_mediapipe_detector(model_path='detector.tflite', min_detection_confidence=0.5):\n","    options = FaceDetectorOptions(\n","        base_options=BaseOptions(model_asset_path=model_path),\n","        running_mode=VisionRunningMode.IMAGE,\n","        min_detection_confidence=min_detection_confidence\n","    )\n","    detector = FaceDetector.create_from_options(options)\n","    return detector\n","\n","\n","def _normalized_to_pixel_coordinates(normalized_x, normalized_y, image_width, image_height):\n","    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n","    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n","    return (x_px, y_px)\n","\n","\n","def detect_faces_mediapipe(detector, image_path):\n","    img = cv2.imread(image_path)\n","    height, width, _ = img.shape\n","    mp_image = mp.Image.create_from_file(image_path)\n","    detection_result = detector.detect(mp_image)\n","    face_data = []\n","\n","    for detection in detection_result.detections:\n","        bbox = detection.bounding_box\n","        center_x = bbox.origin_x + bbox.width / 2\n","        center_y = bbox.origin_y + bbox.height / 2\n","\n","        # Expand the width and height by 1.1x\n","        new_width = bbox.width * 1.1\n","        new_height = bbox.height * 1.1\n","\n","        # Calculate the new top, right, bottom, left coordinates\n","        top = int(center_y - new_height / 2)\n","        right = int(center_x + new_width / 2)\n","        bottom = int(center_y + new_height / 2)\n","        left = int(center_x - new_width / 2)\n","\n","        face_bbox = (top, right, bottom, left)\n","        score = detection.categories[0].score\n","        keypoints = []\n","        for keypoint in detection.keypoints:\n","            keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y, width, height)\n","            keypoints.append(keypoint_px)\n","        face_data.append((image_path, face_bbox, keypoints, score))\n","\n","    return face_data\n","\n","def process_video(video_path, model_path='detector.tflite', min_detection_confidence=0.6, generate_embeddings=True):\n","    detector = initialize_mediapipe_detector(model_path, min_detection_confidence)\n","\n","    cap = cv2.VideoCapture(video_path)\n","    frame_idx = 0\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    face_data_list = []\n","    total_faces = 0\n","    with tqdm(total=total_frames // 2, desc=\"Processing frames\") as pbar:\n","      while cap.isOpened():\n","          ret, frame = cap.read()\n","          if not ret:\n","              break\n","\n","          if frame_idx % 2 == 0:\n","            try:\n","              video_path = os.path.basename(video_path)\n","              video_path = os.path.splitext(video_path)[0]\n","              frame_path = f\"{video_path}_frame{frame_idx:05d}.jpg\"\n","              cv2.imwrite(frame_path, frame)\n","              face_data = detect_faces_mediapipe(detector, frame_path)\n","              face_num = 0  # Counter for faces in the current frame\n","              if face_data:\n","                for face in face_data:\n","                    (image_path, (top, right, bottom, left), keypoints, score) = face\n","                    face_image = frame[top:bottom, left:right]\n","                    if face_image.size != 0:\n","\n","                        face_path = f\"{video_path}_frame{frame_idx:05d}_face{face_num:02d}\"\n","                        embedding = None\n","                        if generate_embeddings:\n","                            embedding = generate_embedding(face_image)\n","                        face_data_list.append((face_path, (top, right, bottom, left), keypoints, score, embedding))\n","                        face_num += 1\n","              total_faces += face_num\n","\n","            finally:\n","              os.remove(frame_path)\n","\n","            pbar.set_description(f\"Processing {video_path} frame {frame_idx // 2 + 1}/{total_frames // 2}\")\n","            pbar.update(1)\n","\n","          frame_idx += 1\n","\n","      cap.release()\n","      print(total_faces)\n","    return face_data_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eWyjjdKRE1-"},"outputs":[],"source":["#ORIGINAL MP\n","def process_video_and_search(video_path, emb_dict, model_path='detector.tflite', min_detection_confidence=0.6, threshold = 0.4, save_n=500):\n","    detector = initialize_mediapipe_detector(model_path, min_detection_confidence)\n","\n","    video_name = os.path.basename(video_path)\n","    video_name = os.path.splitext(video_name)[0]\n","\n","    output_dir = os.path.join(os.path.dirname(video_path), video_name)\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","\n","    checkpoint_path = os.path.join(output_dir, f\"{video_name}_checkpoint.pkl\")\n","    cumulative_matches_path = os.path.join(output_dir, f\"{video_name}_cumulative_matches.csv\")\n","\n","    if os.path.exists(checkpoint_path):\n","        with open(checkpoint_path, 'rb') as cp_file:\n","            checkpoint = pickle.load(cp_file)\n","            frame_idx = checkpoint['frame_idx']\n","            print(f\"Resuming from frame {frame_idx}\")\n","    else:\n","        frame_idx = 0\n","\n","    if os.path.exists(cumulative_matches_path):\n","        cumulative_matches = pd.read_csv(cumulative_matches_path).to_dict('records')\n","    else:\n","        cumulative_matches = []\n","\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    total_faces = 0\n","\n","    with tqdm(total=total_frames, initial=frame_idx, desc=\"Processing frames\") as pbar:\n","      while cap.isOpened():\n","          ret, frame = cap.read()\n","          if not ret:\n","              break\n","\n","          matches = []\n","          try:\n","              video_path = os.path.basename(video_path)\n","              video_path = os.path.splitext(video_path)[0]\n","              frame_path = f\"{video_path}_frame{frame_idx:05d}.jpg\"\n","              cv2.imwrite(frame_path, frame)\n","              face_data = detect_faces_mediapipe(detector, frame_path)\n","              face_num = 0  # Counter for faces in the current frame\n","              if face_data:\n","                #print(\"face \" + str(face_num) + \"in frame: \" + str(frame_idx))\n","                for face in face_data:\n","                    (image_path, (top, right, bottom, left), keypoints, score) = face\n","                    face_image = frame[top:bottom, left:right]\n","                    if face_image is not None and face_image.size != 0:\n","\n","                      vid_embedding = generate_embedding(face_image)\n","                      if vid_embedding is not None:\n","                        match_data = {\n","                            'name': 'NA',\n","                            'badge_number': 'NA',\n","                            'img_name': 'NA',\n","                            'file_name': f\"{video_name}_frame{frame_idx:05d}_face{face_num:02d}\",\n","                            'bbox_coords': (top, right, bottom, left),\n","                            'face_confidence_score': score,\n","                            'similarity': 0,\n","                            'matched_embedding': None\n","                        }\n","                        vid_embedding=vid_embedding[0]\n","                        for emb_index, emb_row in emb_dict.iterrows():\n","                            dict_embedding = torch.tensor(emb_row['embedding'])\n","                            similarity = torch.nn.functional.cosine_similarity(vid_embedding.squeeze(0), dict_embedding.squeeze(0), dim=0)\n","                            if similarity.item() >= threshold:\n","                              print(f\"Match {emb_row['name']} found for frame {frame_idx} with similarity {similarity:.2f}\")\n","                              match_data = {\n","                                  'name': emb_row['name'],\n","                                  'badge_number': emb_row['badge_number'],\n","                                  'img_name': emb_row['source_link'],\n","                                  'file_name': f\"{video_name}_frame{frame_idx:05d}_face{face_num:02d}\",\n","                                  'bbox_coords': (top, right, bottom, left),\n","                                  'face_confidence_score': score,\n","                                  'similarity': similarity.cpu().numpy(),\n","                                  'matched_embedding': vid_embedding\n","                              }\n","                        cumulative_matches.append(match_data)\n","                      face_num += 1\n","                total_faces += face_num\n","\n","                if frame_idx % save_n == 0 and cumulative_matches:\n","                  pd.DataFrame(cumulative_matches).to_csv(cumulative_matches_path, index=False)\n","                  checkpoint = {'frame_idx': frame_idx}\n","                  with open(checkpoint_path, 'wb') as cp_file:\n","                    pickle.dump(checkpoint, cp_file)\n","                  print(f\"Checkpoint saved at frame {frame_idx} with {len(cumulative_matches)} total matches.\")\n","\n","          finally:\n","              os.remove(frame_path)\n","\n","          pbar.set_description(f\"Processing {video_path} frame {frame_idx + 1}/{total_frames}\")\n","          pbar.update(1)\n","\n","\n","          frame_idx += 1\n","\n","      cap.release()\n","      print(f\"Total mp faces detected: {total_faces}\")\n","\n","    # Save final cumulative results\n","    if cumulative_matches:\n","        pd.DataFrame(cumulative_matches).to_csv(cumulative_matches_path, index=False)\n","        print(f\"Total {len(cumulative_matches)} matches saved to {cumulative_matches_path}\")\n","\n","    # Remove checkpoint after processing is complete\n","    if os.path.exists(checkpoint_path):\n","        os.remove(checkpoint_path)\n","\n","    return cumulative_matches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUXosXFmMYbk"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11724,"status":"ok","timestamp":1725376926283,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"u140zXvUUz0r","outputId":"ca1ae2eb-ea4a-4187-8e33-6c3221fe01c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n","Downloading: \"https://github.com/ternaus/retinaface/releases/download/0.01/retinaface_resnet50_2020-07-20-f168fae3c.zip\" to /root/.cache/torch/hub/checkpoints/retinaface_resnet50_2020-07-20-f168fae3c.zip\n","100%|██████████| 96.9M/96.9M [00:02<00:00, 38.6MB/s]\n","/usr/local/lib/python3.10/dist-packages/torch/hub.py:665: UserWarning: Falling back to the old format < 1.6. This support will be deprecated in favor of default zipfile format introduced in 1.6. Please redo torch.save() to save it in the new zipfile format.\n","  warnings.warn('Falling back to the old format < 1.6. This support will be '\n"]}],"source":["#@title **RETINA FACE DETECT**\n","!pip install -U retinaface_pytorch > /dev/null\n","\n","from retinaface.pre_trained_models import get_model\n","from matplotlib import pyplot as plt\n","\n","retfacemodel = get_model(\"resnet50_2020-07-20\", max_size=2048)\n","retfacemodel.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"elapsed":418,"status":"error","timestamp":1725376926696,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"hXBtvMlyR7wu","outputId":"f9bb2d4c-030d-4751-c120-cc7c7ec17623"},"outputs":[{"ename":"error","evalue":"OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a8fab672954e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/_content_drive_MyDrive_COPY_VOL00001_NATIVES_NATIVE00001_CCRB-00001679.MOV_frame_0.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretfacemodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_jsons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Loop through detected faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}],"source":["#test\n","\n","image = cv2.imread(\"/content/_content_drive_MyDrive_COPY_VOL00001_NATIVES_NATIVE00001_CCRB-00001679.MOV_frame_0.jpg\")\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","detections = retfacemodel.predict_jsons(image)\n","\n","# Loop through detected faces\n","for face in detections:\n","    # Get bounding box\n","    facial_area = face['bbox']\n","    # Get landmarks (keypoints)\n","    landmarks = face['landmarks']\n","    # Get confidence score\n","    score = face['score']\n","    #print(f\"Bounding Box: {facial_area}\")\n","    #print(f\"Keypoints: {landmarks}\")\n","    #print(f\"Confidence Score: {score}\\n\")\n","\n","    # Draw bounding box and keypoints on the image\n","    cv2.rectangle(image, (facial_area[0], facial_area[1]), (facial_area[2], facial_area[3]), (0, 255, 0), 2)\n","    for point in landmarks:\n","        # Convert the coordinates to integers\n","        coord = (int(point[0]), int(point[1]))\n","        cv2.circle(image, coord, 2, (0, 0, 255), -1)\n","\n","# Display the image with detected faces\n","plt.imshow(image)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7qU_99IUnWh"},"outputs":[],"source":["import os\n","import pickle\n","import pandas as pd\n","import torch\n","from tqdm.notebook import tqdm\n","from retinaface.pre_trained_models import get_model\n","\n","def detect_faces_retinaface(model, frame_path, min_detection_score):\n","    # Convert frame to RGB as required by the RetinaFace model\n","    image = cv2.imread(frame_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    detections = model.predict_jsons(image)\n","    face_data = []\n","    for face in detections:\n","        score = face['score']\n","        if score < min_detection_score:\n","            continue\n","        # Extract bounding box and landmarks\n","        bbox = face['bbox']\n","        landmarks = face['landmarks']\n","        score = face['score']\n","\n","\n","        face_data.append((frame_path, bbox, landmarks, score))\n","\n","        #cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n","\n","\n","        # Display the image with detected faces\n","    #plt.imshow(image)\n","    #plt.axis('off')\n","    #plt.show()\n","\n","    return face_data\n","total_face_data = []\n","\n","def process_video_and_search(video_path, emb_dict, min_detection_score = 0.95, threshold=0.4, save_n=100):\n","    # Initialize the RetinaFace model\n","    retinaface_model = get_model(\"resnet50_2020-07-20\", max_size=2048)\n","    retinaface_model.eval()\n","\n","    video_name = os.path.basename(video_path)\n","    video_name = os.path.splitext(video_name)[0]\n","\n","    output_dir = os.path.join(os.path.dirname(video_path), video_name)\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    checkpoint_path = os.path.join(output_dir, f\"{video_name}_checkpoint.pkl\")\n","    cumulative_matches_path = os.path.join(output_dir, f\"{video_name}_cumulative_matches.csv\")\n","\n","    if os.path.exists(checkpoint_path):\n","        with open(checkpoint_path, 'rb') as cp_file:\n","            checkpoint = pickle.load(cp_file)\n","            frame_idx = checkpoint['frame_idx']\n","            print(f\"Resuming from frame {frame_idx}\")\n","    else:\n","        frame_idx = 0\n","\n","    if os.path.exists(cumulative_matches_path):\n","        cumulative_matches = pd.read_csv(cumulative_matches_path).to_dict('records')\n","    else:\n","        cumulative_matches = []\n","\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    total_faces = 0\n","    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n","\n","    with tqdm(total=total_frames, initial=frame_idx, desc=\"Processing frames\") as pbar:\n","        while cap.isOpened() and frame_idx < total_frames:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            matches = []\n","            try:\n","                video_path = os.path.basename(video_path)\n","                video_path = os.path.splitext(video_path)[0]\n","                frame_path = f\"{video_name}_frame{frame_idx:05d}.jpg\"\n","                cv2.imwrite(frame_path, frame)\n","\n","                # Use RetinaFace instead of MediaPipe\n","                face_data = detect_faces_retinaface(retinaface_model, frame_path, min_detection_score)\n","                face_num = 0  # Counter for faces in the current frame\n","                if face_data:\n","                    print(\"frame: \" + str(frame_idx) + \" faces: \" + str(len(face_data)))\n","                    total_face_data.append(face_data)\n","                    for face in face_data:\n","                        (image_path, (left, top, right, bottom), keypoints, score) = face\n","\n","                        center_x = (left + right) / 2\n","                        center_y = (top + bottom) / 2\n","                        width = right - left\n","                        height = bottom - top\n","\n","                        new_width = width * 1.5\n","                        new_height = height * 1.5\n","\n","                        left = int(center_x - new_width / 2)\n","                        right = int(center_x + new_width / 2)\n","                        top = int(center_y - new_height / 2)\n","                        bottom = int(center_y + new_height / 2)\n","\n","                        left = max(0, left)\n","                        top = max(0, top)\n","                        right = min(frame.shape[1], right)\n","                        bottom = min(frame.shape[0], bottom)\n","\n","                        face_image = frame[top:bottom, left:right]\n","\n","                        match_data = {\n","                                    'name': 'NA',\n","                                    'badge_number': 'NA',\n","                                    'img_name': 'NA',\n","                                    'file_name': f\"{video_name}_frame{frame_idx:05d}_face{face_num:02d}\",\n","                                    'bbox_coords': (top, right, bottom, left),\n","                                    'face_confidence_score': score,\n","                                    'similarity': 0,\n","                                    'matched_embedding': None\n","                        }\n","                        if face_image is not None and face_image.size != 0:\n","                            vid_embedding = generate_embedding(face_image)\n","                            if vid_embedding is not None:\n","                                vid_embedding = vid_embedding[0]\n","                                for emb_index, emb_row in emb_dict.iterrows():\n","                                    dict_embedding = torch.tensor(emb_row['embedding'])\n","                                    similarity = torch.nn.functional.cosine_similarity(vid_embedding.squeeze(0), dict_embedding.squeeze(0), dim=0)\n","                                    if similarity.item() >= threshold:\n","                                        print(f\"Match {emb_row['name']} found for frame {frame_idx} with similarity {similarity:.2f}\")\n","                                        match_data = {\n","                                            'name': emb_row['name'],\n","                                            'badge_number': emb_row['badge_number'],\n","                                            'img_name': emb_row['source_link'],\n","                                            'file_name': f\"{video_name}_frame{frame_idx:05d}_face{face_num:02d}\",\n","                                            'bbox_coords': (top, right, bottom, left),\n","                                            'face_confidence_score': score,\n","                                            'similarity': similarity.cpu().numpy(),\n","                                            'matched_embedding': vid_embedding\n","                                        }\n","                            face_num += 1\n","                        cumulative_matches.append(match_data)\n","                    total_faces += face_num\n","\n","                    if frame_idx % save_n == 0:\n","                        print(\"cumulative matches list: \" + str(len(cumulative_matches)))\n","                        if cumulative_matches:\n","                          pd.DataFrame(cumulative_matches).to_csv(cumulative_matches_path, index=False)\n","                          checkpoint = {'frame_idx': frame_idx}\n","                          with open(checkpoint_path, 'wb') as cp_file:\n","                              pickle.dump(checkpoint, cp_file)\n","                          print(f\"Checkpoint saved at frame {frame_idx} with {len(cumulative_matches)} total matches.\")\n","\n","            finally:\n","                os.remove(frame_path)\n","\n","            pbar.set_description(f\"Processing {video_path} frame {frame_idx + 1}/{total_frames}\")\n","            pbar.update(1)\n","\n","            frame_idx += 1\n","\n","        cap.release()\n","        print(f\"Total faces detected: {total_faces}\")\n","\n","    # Save final cumulative results\n","    if cumulative_matches:\n","        pd.DataFrame(cumulative_matches).to_csv(cumulative_matches_path, index=False)\n","        print(f\"Total {len(cumulative_matches)} matches saved to {cumulative_matches_path}\")\n","\n","    # Remove checkpoint after processing is complete\n","    if os.path.exists(checkpoint_path):\n","        os.remove(checkpoint_path)\n","\n","    return cumulative_matches\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwwWqLxgeuSU","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b609d21faaa34904b1ecbcc18ac971e4","2f0cec55c6c64066a18b724d2f0bf5cd","12a9abc10d3240b5a55c177d6760ea2c","263b7521663d4485aa8cf50d6bbad07a","98a4d5b49a0347d0b6dc9cb4805f19e9","8ea09e7b8c334a9c9d4a4df5255d707f","9c8eb1403d814adab24cf845855a69e3","8cbdbcf189074b749e0ab8e17a936351","0442f8e4d10947e68971939a593d35df","88278a9ecd5b4406bf8501f0deecec7f","b6e007a5afb7458181bc12bf28f3e8f8"]},"outputId":"35387001-7b9e-4e79-ffc0-1092341ae7d3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Processing frames:   0%|          | 0/5788 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b609d21faaa34904b1ecbcc18ac971e4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["frame: 0 faces: 2\n","cumulative matches list: 2\n","Checkpoint saved at frame 0 with 2 total matches.\n","frame: 1 faces: 2\n","frame: 2 faces: 4\n","frame: 3 faces: 3\n","frame: 4 faces: 5\n","frame: 5 faces: 6\n","frame: 6 faces: 7\n","frame: 7 faces: 4\n","frame: 8 faces: 7\n","frame: 9 faces: 6\n","frame: 10 faces: 6\n","frame: 11 faces: 7\n","frame: 12 faces: 3\n","frame: 13 faces: 8\n","frame: 14 faces: 3\n","frame: 15 faces: 5\n","frame: 16 faces: 4\n","frame: 17 faces: 4\n","frame: 18 faces: 6\n","frame: 19 faces: 2\n","frame: 20 faces: 3\n","frame: 21 faces: 4\n","frame: 22 faces: 2\n","frame: 23 faces: 6\n","frame: 24 faces: 3\n","frame: 25 faces: 6\n","frame: 26 faces: 5\n","frame: 27 faces: 4\n","frame: 28 faces: 2\n","frame: 29 faces: 4\n","frame: 30 faces: 2\n","frame: 31 faces: 4\n","frame: 32 faces: 5\n","frame: 33 faces: 5\n","frame: 34 faces: 5\n","frame: 35 faces: 4\n","frame: 36 faces: 4\n","frame: 37 faces: 4\n","frame: 38 faces: 4\n","frame: 39 faces: 5\n","frame: 40 faces: 4\n","frame: 41 faces: 6\n","frame: 42 faces: 6\n","frame: 43 faces: 5\n","frame: 44 faces: 4\n","frame: 45 faces: 5\n","frame: 46 faces: 2\n","frame: 47 faces: 5\n","frame: 48 faces: 5\n","frame: 49 faces: 6\n","frame: 50 faces: 6\n","frame: 51 faces: 4\n","frame: 52 faces: 8\n","frame: 53 faces: 6\n","frame: 54 faces: 5\n","frame: 55 faces: 8\n","frame: 56 faces: 6\n","frame: 57 faces: 7\n","frame: 58 faces: 6\n","frame: 59 faces: 6\n","frame: 60 faces: 7\n","frame: 61 faces: 6\n","frame: 62 faces: 5\n","frame: 63 faces: 6\n","frame: 64 faces: 5\n","frame: 65 faces: 5\n","frame: 66 faces: 4\n","frame: 67 faces: 4\n","frame: 68 faces: 3\n","frame: 69 faces: 5\n","frame: 70 faces: 4\n","frame: 71 faces: 4\n","frame: 72 faces: 5\n","frame: 73 faces: 4\n","frame: 74 faces: 4\n","frame: 75 faces: 5\n","frame: 76 faces: 6\n","frame: 77 faces: 7\n","frame: 78 faces: 7\n","frame: 79 faces: 5\n","frame: 80 faces: 5\n","frame: 81 faces: 6\n","frame: 82 faces: 4\n","frame: 83 faces: 4\n","frame: 84 faces: 3\n","frame: 85 faces: 3\n","frame: 86 faces: 3\n","frame: 87 faces: 3\n","frame: 88 faces: 5\n","frame: 89 faces: 3\n","frame: 90 faces: 4\n","frame: 91 faces: 4\n","frame: 92 faces: 5\n","frame: 93 faces: 4\n","frame: 94 faces: 4\n","frame: 95 faces: 6\n","frame: 96 faces: 6\n","frame: 97 faces: 5\n","frame: 98 faces: 5\n","frame: 99 faces: 7\n","frame: 100 faces: 6\n","cumulative matches list: 481\n","Checkpoint saved at frame 100 with 481 total matches.\n","frame: 101 faces: 7\n","frame: 102 faces: 4\n","frame: 103 faces: 4\n","frame: 104 faces: 4\n","frame: 105 faces: 3\n","frame: 106 faces: 5\n","frame: 107 faces: 5\n","frame: 108 faces: 6\n","frame: 109 faces: 3\n","frame: 110 faces: 5\n","frame: 111 faces: 4\n","frame: 112 faces: 5\n","frame: 113 faces: 4\n","frame: 114 faces: 4\n","frame: 115 faces: 3\n","frame: 116 faces: 3\n","frame: 117 faces: 3\n","frame: 118 faces: 2\n","frame: 119 faces: 2\n","frame: 120 faces: 3\n","frame: 121 faces: 5\n","frame: 122 faces: 5\n","frame: 123 faces: 5\n","frame: 124 faces: 4\n","frame: 125 faces: 5\n","frame: 126 faces: 5\n","frame: 127 faces: 5\n","frame: 128 faces: 7\n","frame: 129 faces: 6\n","frame: 130 faces: 5\n","frame: 131 faces: 5\n","frame: 132 faces: 6\n","frame: 133 faces: 6\n","frame: 134 faces: 6\n","frame: 135 faces: 5\n","frame: 136 faces: 5\n","frame: 137 faces: 4\n","frame: 138 faces: 4\n","frame: 139 faces: 4\n","frame: 140 faces: 3\n","frame: 141 faces: 3\n","frame: 142 faces: 3\n","frame: 143 faces: 3\n","frame: 144 faces: 4\n","frame: 145 faces: 4\n","frame: 146 faces: 4\n","frame: 147 faces: 4\n","frame: 148 faces: 4\n","frame: 149 faces: 3\n","frame: 150 faces: 4\n","frame: 151 faces: 3\n","frame: 152 faces: 3\n","frame: 153 faces: 4\n","frame: 154 faces: 4\n"]}],"source":["video_path = \"/content/drive/MyDrive/faces/success/timessquare/DEFVID_000000165.MP4\"\n","video_similars = process_video_and_search(video_path, filtered, save_n = 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":763,"status":"ok","timestamp":1724706751723,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"3y17oVkQukdg","outputId":"904ef17d-a2e4-46d2-f298-125a70ca71c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["33183\n","33002\n"]}],"source":["import pandas as pd\n","\n","def remove_non_faces(csv_path, non_faces, output_csv_path=None):\n","    # Load the CSV file\n","    df = pd.read_csv(csv_path)\n","    print(len(df))\n","\n","    # Ensure the 'badge_number' column exists\n","    if 'badge_number' not in df.columns:\n","        raise ValueError(\"The CSV file does not contain a 'badge_number' column.\")\n","\n","    # Remove rows where 'badge_number' is in the non_faces list\n","    df_cleaned = df[~df['badge_number'].isin(non_faces)]\n","    print(len(df_cleaned))\n","\n","    # Optionally save the cleaned DataFrame to a new CSV file\n","    if output_csv_path:\n","        df_cleaned.to_csv(output_csv_path, index=False)\n","\n","    return df_cleaned\n","\n","# Example usage\n","csv_path = '/content/drive/MyDrive/faces/success/DEFVID_000000155/DEFVID_000000155_cumulative_matches3.csv'  # Replace with your CSV file path\n","#non_faces = ['#17032', '#10171', '#13404', '#16688', '#648', '#7320', '#13767', '#2126', '#5207', '#3061']\n","\n","cleaned_df = remove_non_faces(csv_path, non_faces, output_csv_path='/content/drive/MyDrive/faces/success/DEFVID_000000155/DEFVID_000000155_cumulative_matches_cleaned.csv')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6umNnLkukY0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wN-s1LYeumUF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbsaHOGKumRL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMYZtlwpnZ5D"},"outputs":[],"source":["import os\n","import glob\n","import cv2\n","import numpy as np\n","from PIL import Image\n","\n","def extract_filename(file_name):\n","    # Find the position of 'frame' and 'face' in the string\n","    frame_index = file_name.find('_frame')\n","    face_index = file_name.find('_face')\n","\n","    # Extract the filename up to 'frame'\n","    filename = file_name[:frame_index]\n","\n","    # Extract the frame number between 'frame' and 'face'\n","    frame_number = int(file_name[frame_index + 6:face_index])\n","\n","    # Extract the face number after 'face'\n","    face_number = int(file_name[face_index + 5:])\n","\n","    return filename, frame_number, face_number\n","\n","\n","def find_video_path(videos_path, filename):\n","    # Use glob to find any file that matches the filename with any extension\n","    search_pattern = os.path.join(videos_path, filename + '.*')\n","    matching_files = glob.glob(search_pattern)\n","\n","    if matching_files:\n","        # If a matching file is found, return its full path\n","        return matching_files[0]\n","    else:\n","        # If no file is found, return None or handle the error appropriately\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOTVkioBhMli"},"outputs":[],"source":["import pandas as pd\n","\n","def concat_csvs(csv_path1, csv_path2, output_csv_path=None):\n","    # Read the first CSV file into a DataFrame\n","    df1 = pd.read_csv(csv_path1)\n","    len(df1)\n","    # Read the second CSV file into a DataFrame\n","    df2 = pd.read_csv(csv_path2)\n","    len(df2)\n","\n","    # Concatenate the two DataFrames\n","    concatenated_df = pd.concat([df1, df2], ignore_index=True)\n","    len(concatenated_df)\n","    # If an output path is specified, save the concatenated DataFrame to a new CSV file\n","    if output_csv_path:\n","        concatenated_df.to_csv(output_csv_path, index=False)\n","\n","    return concatenated_df\n","\n","csv_path1 = '/content/drive/MyDrive/faces/success/DEFVID_000000155/DEFVID_000000155_cumulative_matches0.csv'\n","csv_path2 = '/content/drive/MyDrive/faces/success/DEFVID_000000155/DEFVID_000000155_cumulative_matches2.csv'\n","output_csv_path = '/content/drive/MyDrive/faces/success/DEFVID_000000155/DEFVID_000000155_cumulative_matches3.csv'\n","\n","concatenated_df = concat_csvs(csv_path1, csv_path2, output_csv_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pteO_HXTEcnJ"},"outputs":[],"source":["\n","\n","import os\n","import cv2\n","import pandas as pd\n","import shutil\n","\n","\n","def draw_bounding_boxes(results_csv, videos_path, images_folder_path):\n","    results_df = pd.read_csv(results_csv)\n","    video_name = os.path.basename(results_csv).replace('_cumulative_matches.csv', '')\n","    csv_directory = os.path.dirname(results_csv)  # Get the directory of the input CSV file\n","\n","    # Create directories for source and marked images in the CSV directory\n","    source_dir = os.path.join(csv_directory, 'source')\n","    marked_dir = os.path.join(csv_directory, 'marked')\n","    os.makedirs(source_dir, exist_ok=True)\n","    os.makedirs(marked_dir, exist_ok=True)\n","\n","    # Extract frame numbers and group by them\n","    results_df['frame_number'] = results_df['file_name'].apply(lambda x: extract_filename(x)[1])\n","\n","    grouped = results_df.groupby('frame_number')\n","\n","    for frame_number, group in grouped:\n","        print(frame_number)\n","        # Since all data is for the same video, just get the video path once\n","        video_path = find_video_path(videos_path, video_name)\n","\n","        if os.path.exists(video_path):\n","            cap = cv2.VideoCapture(video_path)\n","            if cap.isOpened():\n","                # Set the frame position\n","                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n","\n","                # Read the frame\n","                ret, frame = cap.read()\n","                if ret:\n","                    # Save the original frame with the naming convention framenumber.jpg\n","                    original_frame_path = os.path.join(source_dir, f\"{video_name}_{frame_number}.jpg\")\n","                    #cv2.imwrite(original_frame_path, frame)\n","                    original_frame_copy = frame.copy()\n","\n","                    # Draw all bounding boxes for this frame\n","                    for _, row in group.iterrows():\n","                        bbox = eval(row['bbox_coords'])  # Convert string representation of tuple to actual tuple\n","                        img_name = row['img_name']\n","                        name = row['name']\n","\n","\n","                        (top, right, bottom, left) = bbox\n","                        #print(bbox)\n","\n","                        margin = 10\n","                        top = max(0, top - margin)\n","                        left = max(0, left - margin)\n","                        bottom = min(frame.shape[0], bottom + margin)\n","                        right = min(frame.shape[1], right + margin)\n","\n","                        #cropped_frame = frame[top:bottom, left:right]\n","                        cropped_frame = original_frame_copy[top:bottom, left:right]\n","                        height, width = cropped_frame.shape[:2]\n","\n","                        target_width = 256\n","                        scaling_factor = target_width / width\n","                        new_height = int(height * scaling_factor)\n","\n","                        scaled_cropped_frame = cv2.resize(cropped_frame, (target_width, new_height))\n","\n","                        if pd.isna(img_name):\n","                            color = (255, 255, 0)  # Cyan for NaN\n","                            cropped_dir = os.path.join(csv_directory, \"other\")\n","                            os.makedirs(cropped_dir, exist_ok=True)\n","                            cropped_frame_path = os.path.join(cropped_dir, f\"{video_name}_{frame_number}.jpg\")\n","                            #cv2.imwrite(cropped_frame_path, cropped_frame)\n","\n","                        else:\n","                            color = (255, 0, 255)  # YellowGreen otherwise\n","\n","                            # Save the cropped image in a folder based on img_name\n","                            folder_name = name.split()[0][:-1]\n","                            cropped_dir = os.path.join(csv_directory, folder_name)\n","                            os.makedirs(cropped_dir, exist_ok=True)\n","                            img_name_sans_ext = os.path.splitext(img_name)[0]\n","                            cropped_frame_path = os.path.join(cropped_dir, f\"{img_name_sans_ext}_{frame_number}.jpg\")\n","                            #cv2.imwrite(cropped_frame_path, cropped_frame)\n","\n","                            # Find the corresponding image by img_name and copy it if it doesn't already exist\n","                            img_name_path = os.path.join(images_folder_path, img_name)\n","                            dest_img_path = os.path.join(cropped_dir, img_name)\n","                            if os.path.exists(img_name_path) and not os.path.exists(dest_img_path):\n","                                shutil.copy(img_name_path, dest_img_path)\n","\n","\n","                            cropped_frame_with_margin = cv2.copyMakeBorder(\n","                                scaled_cropped_frame, margin, margin, margin, margin, cv2.BORDER_CONSTANT, value=color\n","                            )\n","                            cv2.imwrite(cropped_frame_path, cropped_frame_with_margin)\n","\n","                        #cropped_frame_with_margin = cv2.copyMakeBorder(\n","                        #    scaled_cropped_frame, margin, margin, margin, margin, cv2.BORDER_CONSTANT, value=color\n","                        #)\n","\n","                        #cv2.imwrite(cropped_frame_path, cropped_frame_with_margin)\n","\n","                        cv2.rectangle(frame, (left, top), (right, bottom), color, 4)\n","\n","                    # Save the frame with all bounding boxes in the marked directory\n","                    marked_frame_path = os.path.join(marked_dir, f\"{video_name}_{frame_number}_marked.jpg\")\n","                    cv2.imwrite(marked_frame_path, frame)\n","\n","                else:\n","                    print(f\"Failed to read frame: {frame_number} from video: {video_path}\")\n","            else:\n","                print(f\"Failed to open video: {video_path}\")\n","            cap.release()\n","        else:\n","            print(f\"Video not found: {video_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":346740,"status":"ok","timestamp":1724857065982,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"VUHUFXiClbgO","outputId":"54d8b684-2f76-41d3-86ab-acfb694ed444"},"outputs":[{"name":"stdout","output_type":"stream","text":["327\n","328\n","329\n","606\n","608\n","1032\n","1468\n","1475\n","1476\n","1477\n","1478\n","1479\n","1480\n","1481\n","1482\n","1483\n","1488\n","2113\n","2117\n","2160\n","2161\n","2165\n","2192\n","2242\n","2243\n","2245\n","2246\n","2247\n","2248\n","2314\n","2315\n","2408\n","2410\n","2413\n","2414\n","2415\n","2416\n","2417\n","2418\n","2419\n","2420\n","2427\n","2428\n","2429\n","2614\n","2615\n","2729\n","2789\n","2827\n","2838\n","2868\n","2941\n","3062\n","3072\n","3087\n","3088\n","3089\n","3090\n","3091\n","3092\n","3093\n","3094\n","3097\n","3100\n","3107\n","3111\n","3112\n","3214\n","3227\n","3228\n","3230\n","3231\n","3232\n","3233\n","3234\n","3235\n","3236\n","3237\n","3238\n","3239\n","3240\n","3241\n","3242\n","3243\n","3244\n","3245\n","3246\n","3247\n","3248\n","3249\n","3250\n","3251\n","3252\n","3253\n","3254\n","3255\n","3256\n","3257\n","3258\n","3259\n","3260\n","3261\n","3262\n","3263\n","3264\n","3265\n","3266\n","3267\n","3268\n","3269\n","3270\n","3271\n","3272\n","3273\n","3274\n","3275\n","3276\n","3277\n","3278\n","3279\n","3280\n","3281\n","3282\n","3283\n","3284\n","3285\n","3286\n","3287\n","3288\n","3289\n","3290\n","3291\n","3292\n","3293\n","3294\n","3295\n","3296\n","3297\n","3298\n","3299\n","3300\n","3301\n","3303\n","3304\n","3305\n","3307\n","3311\n","3318\n","3332\n","3398\n","3461\n","3471\n","3472\n","3473\n","3474\n","3475\n","3476\n","3477\n","3478\n","3479\n","3480\n","3481\n","3482\n","3483\n","3484\n","3485\n","3486\n","3487\n","3488\n","3489\n","3490\n","3491\n","3492\n","3493\n","3494\n","3495\n","3496\n","3497\n","3498\n","3499\n","3500\n","3501\n","3502\n","3503\n","3504\n","3505\n","3506\n","3507\n","3508\n","3509\n","3510\n","3511\n","3512\n","3529\n","3530\n","3531\n","3532\n","3533\n","3534\n","3535\n","3536\n","3537\n","3538\n","3539\n","3540\n","3541\n","3542\n","3543\n","3544\n","3545\n","3546\n","3547\n","3548\n","3549\n","3550\n","3551\n","3554\n","3566\n","3568\n","3569\n","3570\n","3572\n","3573\n","3574\n","3575\n","3578\n","3580\n","3581\n","3582\n","3583\n","3584\n","3585\n","3588\n","3589\n","3593\n","3594\n","3599\n","3600\n","3601\n","3602\n","3603\n","3608\n","3609\n","3611\n","3612\n","3613\n","3614\n","3615\n","3617\n","3618\n","3619\n","3620\n","3621\n","3626\n","3627\n","3628\n","3629\n","3630\n","3645\n","3650\n","3689\n","3691\n","3692\n","3693\n","3695\n","3696\n","3752\n","3753\n","3754\n","3755\n","3756\n","3757\n","3758\n","3760\n","3761\n","3762\n","3763\n","3764\n","3765\n","3766\n","3772\n","3773\n","3774\n","3777\n","3778\n","3779\n","3781\n","3782\n","3783\n","3784\n","3785\n","3786\n","3787\n","3788\n","3789\n","3790\n","3791\n","3792\n","3793\n","3794\n","3795\n","3796\n","3797\n","3798\n","3799\n","3800\n","3801\n","3802\n","3803\n","3804\n","3805\n","3806\n","3807\n","3808\n","3809\n","3810\n","3811\n","3812\n","3813\n","3814\n","3815\n","3816\n","3817\n","3818\n","3819\n","3820\n","3821\n","3822\n","3823\n","3824\n","3825\n","3826\n","3827\n","3828\n","3829\n","3830\n","3831\n","3832\n","3833\n","3834\n","3835\n","3863\n","3898\n","3899\n","3900\n","3901\n","3902\n","3903\n","3904\n","3905\n","3906\n","3907\n","3908\n","3910\n","3911\n","3912\n","3914\n","3975\n","3976\n","3978\n","3983\n","4001\n","4003\n","4006\n","4017\n","4019\n","4021\n","4022\n","4023\n","4025\n","4026\n","4028\n","4029\n","4031\n","4043\n","4318\n","4319\n","4320\n","4321\n","4322\n","4323\n","4324\n","4325\n","4326\n","4328\n","4329\n","4330\n","4331\n","4332\n","4333\n","4334\n","4335\n","4337\n","4338\n","4339\n","4341\n","4342\n","4343\n","4439\n","4470\n","4489\n","4490\n","4491\n","4499\n","4501\n","4504\n","4505\n","4506\n","4507\n","4508\n","4509\n","4510\n","4511\n","4512\n","4513\n","4514\n","4520\n","4529\n","4531\n","4532\n","4541\n","4542\n","4547\n","4691\n","4811\n","4891\n","4905\n","4922\n","4925\n","4926\n","4928\n","4929\n","4932\n","4934\n","4969\n","4970\n","4972\n","4973\n","4974\n","4975\n","4976\n","4977\n","4978\n","4979\n","4980\n","4981\n","4982\n","4984\n","4985\n","4986\n","4987\n","4988\n","4994\n","5009\n","5012\n","5015\n","5016\n","5017\n","5018\n","5019\n","5020\n","5021\n","5022\n","5023\n","5024\n","5025\n","5026\n","5027\n","5028\n","5037\n","5038\n","5039\n","5045\n","5056\n","5057\n","5095\n","5097\n","5098\n","5099\n","5109\n","5114\n","5128\n","5149\n","5157\n","5159\n","5166\n","5170\n","5172\n","5175\n","5176\n","5180\n","5181\n","5186\n","5188\n","5189\n","5190\n","5191\n","5192\n","5193\n","5197\n","5201\n","5204\n","5206\n","5218\n","5235\n","5247\n","5248\n","5249\n","5250\n","5251\n","5252\n","5262\n","5263\n","5264\n","5267\n","5270\n","5273\n","5283\n","5293\n","5294\n","5295\n","5296\n","5297\n","5298\n","5299\n","5300\n","5301\n","5302\n","5303\n","5304\n","5305\n","5306\n","5307\n","5308\n","5309\n","5310\n","5311\n","5312\n","5313\n","5314\n","5315\n","5316\n","5318\n","5321\n","5324\n","5325\n","5326\n","5327\n","5328\n","5330\n","5331\n","5332\n","5336\n","5337\n","5345\n","5346\n","5348\n","5349\n","5350\n","5351\n","5353\n","5354\n","5355\n","5356\n","5357\n","5358\n","5359\n","5360\n","5362\n","5363\n","5364\n","5365\n","5366\n","5367\n","5368\n","5369\n","5370\n","5371\n","5374\n","5375\n","5376\n","5377\n","5378\n","5382\n","5385\n","5386\n","5387\n","5388\n","5389\n","5392\n","5393\n","5396\n","5397\n","5398\n","5399\n","5400\n","5438\n","5448\n","5458\n","5459\n","5463\n","5464\n","5465\n","5466\n","5469\n","5470\n","5471\n","5486\n","5488\n","5489\n","5500\n","5503\n","5504\n","5519\n","5520\n","5523\n","5524\n","5529\n","5532\n","5536\n","5537\n","5538\n","5539\n","5540\n","5541\n","5542\n","5543\n","5544\n","5545\n","5546\n","5563\n","5564\n","5565\n","5568\n","5572\n","5593\n","5594\n","5595\n","5596\n","5602\n","5604\n","5605\n","5607\n","5608\n","5609\n","5610\n","5612\n","5623\n","5624\n","5629\n","5630\n","5631\n","5632\n","5633\n","5634\n","5636\n","5638\n","5639\n","5643\n","5646\n","5648\n","5650\n","5653\n","5663\n","5664\n","5665\n","5666\n","5669\n","5670\n","5672\n","5673\n","5674\n","5675\n","5682\n","5683\n","5684\n","5687\n","5690\n","5691\n","5692\n","5693\n","5696\n","5699\n","5707\n","5709\n","5710\n","5711\n","5722\n","5975\n","6070\n","6078\n","6079\n","6082\n","6085\n","6086\n","6087\n","6088\n","6089\n","6090\n","6091\n","6092\n","6093\n","6096\n","6097\n","6200\n","6203\n","6214\n","6257\n","6258\n","6371\n","6372\n","6373\n","6374\n","6375\n","6376\n","6377\n","6380\n","6383\n","6384\n","6387\n","6388\n","6389\n","6413\n","6414\n","6415\n","6416\n","6417\n","6418\n","6419\n","6420\n","6421\n","6422\n","6429\n","6430\n","6431\n","6432\n","6433\n","6434\n","6435\n","6437\n","6438\n","6446\n","6447\n","6448\n","6449\n","6450\n","6451\n","6452\n","6453\n","6454\n","6455\n","6456\n","6457\n","6458\n","6459\n","6462\n","6577\n","6578\n","6579\n","6594\n","6595\n","6596\n","6597\n","6598\n","6603\n","6605\n","6611\n","6612\n"]}],"source":["videos_path='/content/drive/MyDrive/faces/success'\n","images_folder_path = '/content/drive/MyDrive/faces/NYPD_profile_imgs'\n","results_csv = '/content/drive/MyDrive/faces/success/DEF_000321472/DEF_000321472_cumulative_matches.csv'\n","\n","draw_bounding_boxes(results_csv, videos_path, images_folder_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFIGMQqQlbdJ"},"outputs":[],"source":["import cv2\n","import os\n","\n","def replace_frames_in_video(original_video_path, marked_frames_dir, output_video_path):\n","    # Open the original video\n","    cap = cv2.VideoCapture(original_video_path)\n","    if not cap.isOpened():\n","        print(f\"Error: Could not open video {original_video_path}\")\n","        return\n","\n","    # Get video properties\n","    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    # Define the codec and create a VideoWriter object to save the new video\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n","\n","    # Load all marked frames into a dictionary keyed by frame number\n","    marked_frames = {}\n","    for file_name in os.listdir(marked_frames_dir):\n","        if file_name.endswith('_marked.jpg'):\n","            # Extract frame number from the file name\n","            frame_number = int(file_name.split('_')[-2])\n","            marked_frame_path = os.path.join(marked_frames_dir, file_name)\n","            marked_frame = cv2.imread(marked_frame_path)\n","            marked_frames[frame_number] = marked_frame\n","\n","    # Process the video and replace frames\n","    for i in range(total_frames):\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Check if there is a marked frame for the current frame number\n","        if i in marked_frames:\n","            frame = marked_frames[i]\n","\n","        # Write the frame to the output video\n","        out.write(frame)\n","\n","    # Release resources\n","    cap.release()\n","    out.release()\n","\n","    print(f\"Finished processing. The video with marked frames has been saved to {output_video_path}\")\n","\n","# Example usage:\n","# replace_frames_in_video('original_video.mp4', 'path_to_marked_frames', 'output_video.mp4')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":598708,"status":"ok","timestamp":1725555876291,"user":{"displayName":"Karyn Nakamura","userId":"07789206291232155731"},"user_tz":240},"id":"sjGKKXnXlbac","outputId":"5c25554f-32c1-4fd3-bc7b-0155a517b638"},"outputs":[{"output_type":"stream","name":"stdout","text":["Finished processing. The video with marked frames has been saved to /content/drive/MyDrive/faces/success/timessquare/DEFVID_000000111/DEFVID_000000111_recomp.mp4\n"]}],"source":["original_video_path = \"/content/drive/MyDrive/faces/success/timessquare/DEFVID_000000111.MP4\"\n","marked_frames_dir = \"/content/drive/MyDrive/faces/success/timessquare/DEFVID_000000111/marked\"\n","output_video_path = \"/content/drive/MyDrive/faces/success/timessquare/DEFVID_000000111/DEFVID_000000111_recomp.mp4\"\n","replace_frames_in_video(original_video_path, marked_frames_dir, output_video_path)"]},{"cell_type":"code","source":[],"metadata":{"id":"TK8BKeYWeVnJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import pandas as pd\n","\n","def draw_bounding_boxes_on_video(results_csv, videos_path, output_video_path):\n","    results_df = pd.read_csv(results_csv)\n","    video_name = os.path.basename(results_csv).replace('_cumulative_matches.csv', '')\n","\n","    # Extract frame numbers and group by them\n","    results_df['frame_number'] = results_df['file_name'].apply(lambda x: extract_filename(x)[1])\n","    grouped = results_df.groupby('frame_number')\n","\n","    # Find the video path\n","    video_path = find_video_path(videos_path, video_name)\n","\n","    if os.path.exists(video_path):\n","        cap = cv2.VideoCapture(video_path)\n","        if not cap.isOpened():\n","            print(f\"Failed to open video: {video_path}\")\n","            return\n","\n","        # Get video properties\n","        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","        # Prepare the output video writer\n","        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","        out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n","\n","        # Process each frame\n","        frame_number = 0\n","        while frame_number < total_frames:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            # If there are bounding boxes for this frame, draw them\n","            if frame_number in grouped.groups:\n","                group = grouped.get_group(frame_number)\n","                for _, row in group.iterrows():\n","                    bbox = eval(row['bbox_coords'])  # Convert string representation of tuple to actual tuple\n","                    name = row['name']\n","                    (top, right, bottom, left) = bbox\n","\n","                    # Add margin\n","                    margin = 10\n","                    top = max(0, top - margin)\n","                    left = max(0, left - margin)\n","                    bottom = min(frame.shape[0], bottom + margin)\n","                    right = min(frame.shape[1], right + margin)\n","\n","                    # Choose color based on whether img_name is NaN\n","                    color = (255, 255, 0) if pd.isna(row['img_name']) else (255, 0, 255)\n","\n","                    # Draw bounding box on the frame\n","                    cv2.rectangle(frame, (left, top), (right, bottom), color, 4)\n","\n","            # Write the frame with bounding boxes to the output video\n","            out.write(frame)\n","\n","            frame_number += 1\n","\n","        # Release the video capture and writer objects\n","        cap.release()\n","        out.release()\n","        print(f\"Video saved at: {output_video_path}\")\n","    else:\n","        print(f\"Video not found: {video_path}\")\n"],"metadata":{"id":"BizgEaYdeViE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["49a89bd9d5ca476daf22225116334a8d","da2744cc28c6404eb58826b3705eb260","f7c460d684ce4973bd700abf6b75c579","829acb3a9edf41478cccedef6e7941c2","e6d73589359c4b9dacf022a20879406e","67c23cea58e74c89b5c48a7bdec7c768","94e434b1439144c98c9fb39b55799592","fc1642ab9d5949c485e1b598b5f6d7a8","874fe238a8ad4327ba2818ba7cb634f3","b6e1cad3efdd4982a9b77368e7ca6d96","39be113486674d97a520f90d0f59c79b","61cadb197bae43369fd05c3ceceeed37","a8ed5d1e6dc14ddd89598b5d32c39977","34d85f80b4424450ba912e1edbca8ed3","e026268f7302479d98fdf27c1b90835d","8eb115d0282543b8b422afc3cadec578","b6a96a3c62fc4fef9b085df96a8fc5e7","307280d56a3447b085772eb68c7b896b","164fdfb79d434f1a966197fc160cc138","0c4c2b048bbb468da6f20e691461b38a","a60eb1007ce54234adcec479f1a7797e","478befbe7df64f62af79c1ea64fd0620","1284b2df23354baaa83214e60609a1b1","1f55e37c55a64e9398467c273c5c8eed","3073d1df598c47578bea4cdcae5230c9","8efc12eb0e1743b7b4e50f1607a4406e","4858ca9da0584d308d43383b1bfd3b71","c07df0a0f8fd4717b06ea546d2a947e7","0737e3ea1c474e12a2da0539bcf27e70","6740e01590ce4c2ebd6e3b785fc555c9","18a6330eddb44b9ab03372491bc444d7","54810d811f6f42fb8132c3c8aac99322","a76651cc027347f2bc8544bbff90c2f2","22aa0cc60dc1432f8a9c4197dca56142","e314d45e1cc84e3baed0c8e5c6072958","5ed82458e0fe47c4bfe1ec616614022a","5e0281aa0b6a4b769025d54bf095612c","7739f5417fa04a8db3a2f62b81d124d4","85a8b8440a5e430f9f9cad9ac8edba6c","525c0cfeb0fc4d65b2151893a69122e9","fd18b7fdc8d44ddab1c2aa38286c47e3","733cdd3cf7954e6cb64262bd04ce7066","f6a2b5c7fd5c4137ba80d3b0ee3a7449","b6908decef8d478d902d15071a0b0ea0","d72b2ae5cfc148318dbcfca911ad34e8","92b06bf066bc4a278183ae37f4a3968c","8b9c141b473e4b66b22044b3b02589d0","109cb1d402a34d00b40fee8a933a86c0","583bd61dd4094741a155b21151b954f5","a937d709baff48528775113c47c1598e","d3c4984a7af846c59ecc2d4d8d974f2d","723d1289b6ba4aa9bae1fd0bbf72ce45","fb5db296c4b04076bb6ce27d9d6748ba","bbdbe343748d415aa58993b355248b90","a3de9cb87ea544c1bb70021f036c4f1c","2a2fbb101c714b48a8e0d1af84c5d50f","45d39f4ff28243e78611d99f235a51f2","c1855d4645a642d28c7dfe9ba6abaf40","372478435de34ce88d610fe1eaf509e5","4e6dabe9c1a74589b3f8f8733a7469c7","9fe6d518e80648a59ada52786e4bea25","17cf1c2b496e4fa0b1dcbd86a794a0b7","832e98dac9ed4e1c86ff779b9c9b5f13","796705339bf247379d72cf9f53d66242","916f9e94f66046d880a866862a44a53a","406a1b00d18c48b8a1e7c46c5e690564","f23c51beec374092ba9ca003f486472d","f65338adaef24c66b8a84cb8bf5283d3","ab6469a433d3418eb14e01a6190590c6","45997830992f43ffb94bc100c9c9aa02","c08956c02f8d455e830689debb622373","85275e01e8774bf1bde8d2cc7ff27eac","bda6797a8deb45279d7673aae28fdbc4","4552403d9c124ce28f4ee84c982866e4","dfd65251584945ceb5c36c7ff1b421c7","e506d2bb60a14434b51cbd3db4efc3fa","09a970fc21dc4af185b9c15273c8db21","2c27010ecbf74d08b9620c4747067961","6d0ee29fea254fceacdc457d128ebe1e","617dba6b31a34939ab8760994d55e47f","28a3cdddc3d048459a6ec26cc710509b","fc81e4ac8d7e4aac9da04459c2d4d178","a9188df102f74a0b97068c21f23024b4","79ba5e1c91a6443eb6b8f47ccd3c68eb","1481e8ad01f44494af407b260f530db7","ca7f831e2d804957ba3a3006ae3775a0","596004c071384a1b8eb4cba605daf067","ae4cd73f323d488abbea39c47ca188aa","fcaf8b37df434540b78e9a971086605b","33bc4aed36a5491290e6cc3eff89efa6","b5582e053bc8433bbc67cc6435e19913","b3affe23d00a4d14a82ea4561c186b43","ab57b7658c354c88ba215b862887cab4","3562be7db56747bd955b6d5a21b2e7a1","8789ce123a1b4497956e156e965360a6","e590ca19354a481684262c811fc0c655","4abda06dc851478a8329efd0a6499712","4910a61133224323afe32cb7ea66733f","a23d3c84538e47febe18b491d2bfcdf1","b45244568ad54ea2b70e48a55c2a20c8","801dcae5d16c4cf093d12265fd11e0ef","2d59cedaccdf45c793ce0e65779d31ca","5e91989a46964a2f8527699ea4294ee5","89f9a2a2c2814688873feabea68b2459","9fe7f5f00e5a443ca953acb91bfffecc"]},"collapsed":true,"id":"w40Z2iGRl5jB","outputId":"f31b9e99-0ae1-4fdb-e194-3c596528af80"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49a89bd9d5ca476daf22225116334a8d","version_major":2,"version_minor":0},"text/plain":["Processing videos:   0%|          | 0/1043 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271487.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da2744cc28c6404eb58826b3705eb260","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/1041 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n","  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"]},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 31\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271487.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271488.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7c460d684ce4973bd700abf6b75c579","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/152 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 0\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271488.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/NYAG-G-00001031.0001.MP4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"829acb3a9edf41478cccedef6e7941c2","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/419 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n","To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n","  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"]},{"name":"stdout","output_type":"stream","text":["Match Kasaji, Amjad K. found for frame 576 with similarity 0.40\n","Total mp faces detected: 8\n","Total 1 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/NYAG-G-00001031.0001_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/NYAG-G-00001031.0001.MP4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271489.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6d73589359c4b9dacf022a20879406e","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/160 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 2\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271489.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271490.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67c23cea58e74c89b5c48a7bdec7c768","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/1090 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 19\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271490.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271491.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94e434b1439144c98c9fb39b55799592","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/945 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 103\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271491.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271492.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc1642ab9d5949c485e1b598b5f6d7a8","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/1018 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Dill, Billy R. found for frame 1354 with similarity 0.43\n","Total mp faces detected: 441\n","Total 1 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000271492_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271492.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271495.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"874fe238a8ad4327ba2818ba7cb634f3","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/338 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 23\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271495.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271493.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6e1cad3efdd4982a9b77368e7ca6d96","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/497 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 2\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271493.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271497.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39be113486674d97a520f90d0f59c79b","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/238 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 18\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271497.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271496.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61cadb197bae43369fd05c3ceceeed37","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/388 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 16\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271496.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271494.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8ed5d1e6dc14ddd89598b5d32c39977","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/1213 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 3\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271494.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/NYPD-0000114533.MP4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34d85f80b4424450ba912e1edbca8ed3","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/18112 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Owens, Thomas A. found for frame 23088 with similarity 0.42\n","Match Carolei, Anthony J. found for frame 23754 with similarity 0.42\n","Match Carolei, Anthony J. found for frame 23996 with similarity 0.52\n","Match Carolei, Anthony J. found for frame 23998 with similarity 0.51\n","Match Wybraniec, Mateusz J. found for frame 27960 with similarity 0.42\n","Match Londono, Andres N. found for frame 28430 with similarity 0.50\n","Match Walsh, Ashley L. found for frame 28450 with similarity 0.46\n","Match Hicks, Brandis L. found for frame 28450 with similarity 0.41\n","Match Carolei, Anthony J. found for frame 30854 with similarity 0.45\n","Total mp faces detected: 631\n","Total 9 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/NYPD-0000114533_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/NYPD-0000114533.MP4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271498.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e026268f7302479d98fdf27c1b90835d","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/348 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 23\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271498.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271504.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8eb115d0282543b8b422afc3cadec578","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/179 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 8\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271504.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271503.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6a96a3c62fc4fef9b085df96a8fc5e7","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/476 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 0\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271503.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271516.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"307280d56a3447b085772eb68c7b896b","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/238 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 19\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271516.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271499.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"164fdfb79d434f1a966197fc160cc138","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/582 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Londono, Andres N. found for frame 430 with similarity 0.41\n","Match Hicks, Brandis L. found for frame 430 with similarity 0.41\n","Total mp faces detected: 21\n","Total 2 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000271499_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271499.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271522.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c4c2b048bbb468da6f20e691461b38a","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/278 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 4\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271522.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271502.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a60eb1007ce54234adcec479f1a7797e","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/710 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 6\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271502.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271536.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"478befbe7df64f62af79c1ea64fd0620","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/307 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 4\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271536.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271570.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1284b2df23354baaa83214e60609a1b1","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/444 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 33\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271570.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271501.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f55e37c55a64e9398467c273c5c8eed","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/2099 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 0\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271501.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271569.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3073d1df598c47578bea4cdcae5230c9","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/209 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 13\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271569.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271550.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8efc12eb0e1743b7b4e50f1607a4406e","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/472 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 2\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271550.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271551.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4858ca9da0584d308d43383b1bfd3b71","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/356 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 271\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271551.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271545.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c07df0a0f8fd4717b06ea546d2a947e7","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/731 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 6\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271545.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271552.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0737e3ea1c474e12a2da0539bcf27e70","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/321 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 5\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271552.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271535.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6740e01590ce4c2ebd6e3b785fc555c9","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/830 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 8\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271535.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271574.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18a6330eddb44b9ab03372491bc444d7","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/444 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 30\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271574.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271548.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54810d811f6f42fb8132c3c8aac99322","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/675 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 206\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271548.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271579.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a76651cc027347f2bc8544bbff90c2f2","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/151 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 0\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271579.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271597.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22aa0cc60dc1432f8a9c4197dca56142","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/89 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 2\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271597.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271587.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e314d45e1cc84e3baed0c8e5c6072958","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/708 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 14\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271587.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271911.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ed82458e0fe47c4bfe1ec616614022a","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/161 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271911.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271585.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e0281aa0b6a4b769025d54bf095612c","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/338 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 17\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271585.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271687.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7739f5417fa04a8db3a2f62b81d124d4","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/189 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271687.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271922.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85a8b8440a5e430f9f9cad9ac8edba6c","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/115 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 3\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271922.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271923.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"525c0cfeb0fc4d65b2151893a69122e9","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/75 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 0\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000271923.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272059.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd18b7fdc8d44ddab1c2aa38286c47e3","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/740 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 2\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272059.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272058.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"733cdd3cf7954e6cb64262bd04ce7066","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/752 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 15\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272058.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272073.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6a2b5c7fd5c4137ba80d3b0ee3a7449","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/547 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272073.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272062.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6908decef8d478d902d15071a0b0ea0","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/617 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 5\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272062.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272061.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d72b2ae5cfc148318dbcfca911ad34e8","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/675 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 12\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272061.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272060.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92b06bf066bc4a278183ae37f4a3968c","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/598 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Londono, Andres N. found for frame 752 with similarity 0.42\n","Total mp faces detected: 93\n","Total 1 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000272060_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272060.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272080.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b9c141b473e4b66b22044b3b02589d0","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/2098 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Hicks, Brandis L. found for frame 4130 with similarity 0.47\n","Total mp faces detected: 120\n","Total 1 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000272080_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272080.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272081.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"109cb1d402a34d00b40fee8a933a86c0","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/2098 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 94\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272081.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272084.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"583bd61dd4094741a155b21151b954f5","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/228 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272084.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272092.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a937d709baff48528775113c47c1598e","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 10\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272092.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272082.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3c4984a7af846c59ecc2d4d8d974f2d","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/2098 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 97\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272082.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272083.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"723d1289b6ba4aa9bae1fd0bbf72ce45","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/2099 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 93\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272083.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272091.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb5db296c4b04076bb6ce27d9d6748ba","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/710 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 29\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272091.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272085.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bbdbe343748d415aa58993b355248b90","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/893 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 12\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272085.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272121.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3de9cb87ea544c1bb70021f036c4f1c","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/587 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272121.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272105.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a2fbb101c714b48a8e0d1af84c5d50f","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/676 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 46\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272105.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272100.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45d39f4ff28243e78611d99f235a51f2","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/676 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 48\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272100.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272122.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1855d4645a642d28c7dfe9ba6abaf40","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/676 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 46\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272122.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272244.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"372478435de34ce88d610fe1eaf509e5","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/283 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 13\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272244.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272123.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e6dabe9c1a74589b3f8f8733a7469c7","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/1439 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Hicks, Brandis L. found for frame 2260 with similarity 0.41\n","Match Hicks, Brandis L. found for frame 2264 with similarity 0.50\n","Match Hicks, Brandis L. found for frame 2340 with similarity 0.40\n","Match Hicks, Brandis L. found for frame 2342 with similarity 0.55\n","Match Hicks, Brandis L. found for frame 2346 with similarity 0.47\n","Total mp faces detected: 113\n","Total 5 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000272123_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272123.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272157.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fe6d518e80648a59ada52786e4bea25","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/1330 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 11\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272157.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272124.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17cf1c2b496e4fa0b1dcbd86a794a0b7","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/1915 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Cunningham, Martin A. 18 3 2020 found for frame 974 with similarity 0.43\n","Match Cunningham, Martin A. 18 3 2020 found for frame 976 with similarity 0.50\n","Match Cunningham, Martin A. 18 3 2020 found for frame 984 with similarity 0.54\n","Match Cunningham, Martin A. 18 3 2020 found for frame 986 with similarity 0.52\n","Match Cunningham, Martin A. 18 3 2020 found for frame 988 with similarity 0.53\n","Match Cunningham, Martin A. 18 3 2020 found for frame 992 with similarity 0.55\n","Match Cunningham, Martin A. 18 3 2020 found for frame 994 with similarity 0.51\n","Checkpoint saved at frame 1500 with 7 total matches.\n","Checkpoint saved at frame 2500 with 7 total matches.\n","Checkpoint saved at frame 3000 with 7 total matches.\n","Checkpoint saved at frame 3500 with 7 total matches.\n","Match Londono, Andres N. found for frame 3748 with similarity 0.42\n","Match Hicks, Brandis L. found for frame 3748 with similarity 0.41\n","Total mp faces detected: 631\n","Total 9 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000272124_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272124.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272316.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"832e98dac9ed4e1c86ff779b9c9b5f13","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/584 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 9\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272316.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272129.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"796705339bf247379d72cf9f53d66242","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/452 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 29\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272129.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272215.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"916f9e94f66046d880a866862a44a53a","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/2105 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Hicks, Brandis L. found for frame 3258 with similarity 0.48\n","Total mp faces detected: 278\n","Total 1 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000272215_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272215.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272349.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"406a1b00d18c48b8a1e7c46c5e690564","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/175 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 0\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272349.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272315.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f23c51beec374092ba9ca003f486472d","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/1628 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 43\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272315.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272477.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f65338adaef24c66b8a84cb8bf5283d3","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/295 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 3\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272477.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272416.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab6469a433d3418eb14e01a6190590c6","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/215 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 14\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272416.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272478.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45997830992f43ffb94bc100c9c9aa02","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/207 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 21\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272478.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272317.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c08956c02f8d455e830689debb622373","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/2107 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Londono, Andres N. found for frame 958 with similarity 0.43\n","Total mp faces detected: 282\n","Total 1 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000272317_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272317.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272415.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85275e01e8774bf1bde8d2cc7ff27eac","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/699 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 22\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272415.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272479.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bda6797a8deb45279d7673aae28fdbc4","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/415 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Londono, Andres N. found for frame 194 with similarity 0.50\n","Match Hicks, Brandis L. found for frame 194 with similarity 0.48\n","Match Londono, Andres N. found for frame 222 with similarity 0.52\n","Match Hicks, Brandis L. found for frame 222 with similarity 0.46\n","Total mp faces detected: 170\n","Total 4 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000272479_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272479.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272482.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4552403d9c124ce28f4ee84c982866e4","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/239 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 34\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272482.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272758.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfd65251584945ceb5c36c7ff1b421c7","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/145 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 34\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272758.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272728.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e506d2bb60a14434b51cbd3db4efc3fa","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/289 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 21\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272728.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272759.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09a970fc21dc4af185b9c15273c8db21","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/208 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 2\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272759.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272480.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c27010ecbf74d08b9620c4747067961","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/908 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 31\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272480.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272318.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d0ee29fea254fceacdc457d128ebe1e","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/1590 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Scanlon, Clifford J. found for frame 884 with similarity 0.40\n","Match Scanlon, Clifford J. found for frame 886 with similarity 0.40\n","Match Scanlon, Clifford J. found for frame 888 with similarity 0.42\n","Match Cunningham, Martin A. 18 3 2020 found for frame 1200 with similarity 0.48\n","Match Cunningham, Martin A. 18 3 2020 found for frame 1210 with similarity 0.50\n","Match Cunningham, Martin A. 18 3 2020 found for frame 1212 with similarity 0.52\n","Match Cunningham, Martin A. 18 3 2020 found for frame 1216 with similarity 0.46\n","Match Cunningham, Martin A. 18 3 2020 found for frame 1218 with similarity 0.47\n","Match Cunningham, Martin A. 18 3 2020 found for frame 1220 with similarity 0.44\n","Checkpoint saved at frame 1500 with 9 total matches.\n","Match Hicks, Brandis L. found for frame 2162 with similarity 0.47\n","Total mp faces detected: 501\n","Total 10 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000272318_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272318.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272760.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"617dba6b31a34939ab8760994d55e47f","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/279 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 18\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272760.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272761.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28a3cdddc3d048459a6ec26cc710509b","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/198 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 10\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272761.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272915.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc81e4ac8d7e4aac9da04459c2d4d178","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/211 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272915.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273255.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9188df102f74a0b97068c21f23024b4","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/412 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273255.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272481.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79ba5e1c91a6443eb6b8f47ccd3c68eb","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/2070 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Hicks, Brandis L. found for frame 80 with similarity 0.41\n","Match Hicks, Brandis L. found for frame 124 with similarity 0.42\n","Match Hicks, Brandis L. found for frame 2136 with similarity 0.47\n","Match Harrison, Judith R. 0 0 found for frame 3468 with similarity 0.40\n","Checkpoint saved at frame 3500 with 4 total matches.\n","Total mp faces detected: 549\n","Total 4 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000272481_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272481.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272979.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1481e8ad01f44494af407b260f530db7","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/738 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 20\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000272979.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273256.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca7f831e2d804957ba3a3006ae3775a0","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/743 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273256.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273269.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"596004c071384a1b8eb4cba605daf067","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/4198 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 468\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273269.mp4\n","Skipping long video /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/NYPD-0000114532.MP4 (total frames: 140638)\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/NYPD-0000114532.MP4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/NYPD-0000115414.MP4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae4cd73f323d488abbea39c47ca188aa","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/69 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/NYPD-0000115414.MP4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/NYPD-0000115415.MP4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fcaf8b37df434540b78e9a971086605b","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/69 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/NYPD-0000115415.MP4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273912.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33bc4aed36a5491290e6cc3eff89efa6","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/405 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 47\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273912.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273906.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5582e053bc8433bbc67cc6435e19913","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/970 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 125\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273906.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273901.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3affe23d00a4d14a82ea4561c186b43","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/495 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 3\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273901.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273897.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab57b7658c354c88ba215b862887cab4","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/863 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Hicks, Brandis L. found for frame 626 with similarity 0.42\n","Total mp faces detected: 55\n","Total 1 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000273897_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273897.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273931.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3562be7db56747bd955b6d5a21b2e7a1","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/604 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 18\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273931.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273920.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8789ce123a1b4497956e156e965360a6","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/234 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 0\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273920.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273940.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e590ca19354a481684262c811fc0c655","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/234 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 0\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273940.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273924.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4abda06dc851478a8329efd0a6499712","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/501 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 108\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273924.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273917.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4910a61133224323afe32cb7ea66733f","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/247 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 10\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273917.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273948.mp4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a23d3c84538e47febe18b491d2bfcdf1","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/767 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Match Londono, Andres N. found for frame 766 with similarity 0.55\n","Match Hicks, Brandis L. found for frame 766 with similarity 0.58\n","Match Londono, Andres N. found for frame 1496 with similarity 0.60\n","Match Hicks, Brandis L. found for frame 1496 with similarity 0.42\n","Checkpoint saved at frame 1500 with 4 total matches.\n","Total mp faces detected: 28\n","Total 4 matches saved to /content/drive/MyDrive/faces/search_output/VOL22/DEF_000273948_cumulative_matches.pkl\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/DEF_000273948.mp4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000016.MP4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b45244568ad54ea2b70e48a55c2a20c8","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/106 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 8\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000016.MP4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000068.MP4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"801dcae5d16c4cf093d12265fd11e0ef","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/231 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 22\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000068.MP4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000056.MP4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d59cedaccdf45c793ce0e65779d31ca","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/156 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 1\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000056.MP4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000055.MP4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e91989a46964a2f8527699ea4294ee5","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/275 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 5\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000055.MP4\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000064.MOV\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89f9a2a2c2814688873feabea68b2459","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/450 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total mp faces detected: 2\n","Checkpoint saved after processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000064.MOV\n","Processing video: /content/drive/MyDrive/COPY/VOL00002/NATIVES/NATIVE00001/Payne-Ctrl-00000071.MP4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fe7f5f00e5a443ca953acb91bfffecc","version_major":2,"version_minor":0},"text/plain":["Processing frames:   0%|          | 0/6728 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def find_all_videos(input_folder):\n","    video_extensions = ['.mp4', '.avi', '.mov', '.m4v','.mpeg']\n","    video_files = []\n","    for root, dirs, files in os.walk(input_folder):\n","        for file in files:\n","            # Convert the file name to lowercase before checking the extension\n","            if any(file.lower().endswith(ext) for ext in video_extensions):\n","                video_files.append(os.path.join(root, file))\n","    return video_files\n","\n","\n","\n","def run_directory(input_folder, emb_dict, output_dir, model_path='detector.tflite', min_detection_confidence=0.6, threshold=0.4, save_n=100):\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    meta_checkpoint_path = os.path.join(output_dir, \"directory_checkpoint.pkl\")\n","\n","    # Load meta checkpoint if it exists\n","    if os.path.exists(meta_checkpoint_path):\n","        with open(meta_checkpoint_path, 'rb') as meta_file:\n","            meta_checkpoint = pickle.load(meta_file)\n","            processed_videos = meta_checkpoint.get('processed_videos', [])\n","            long_videos = meta_checkpoint.get('long_videos', [])\n","\n","            print(f\"Resuming from checkpoint, {len(processed_videos)} videos already processed.\")\n","    else:\n","        processed_videos = []\n","        long_videos = []\n","        meta_checkpoint = {}\n","\n","    video_files = find_all_videos(input_folder)\n","\n","    processed_count = 0\n","\n","    with tqdm(total=len(video_files), desc=\"Processing videos\") as pbar:\n","        for video_file in video_files:\n","          if os.path.abspath(video_file) in processed_videos or os.path.abspath(video_file) in long_videos:\n","                print(f\"Skipping already processed video: {video_file}\")\n","                pbar.update(1)  # Update the progress bar for skipped videos\n","                processed_count+=1\n","                print(processed_count)\n","                continue\n","\n","          cap = cv2.VideoCapture(video_file)\n","          total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","          cap.release()\n","\n","          if total_frames > 108000:\n","                print(f\"Skipping long video {video_file} (total frames: {total_frames})\")\n","                long_videos.append(os.path.abspath(video_file))  # Add skipped video to checkpoint\n","          else:\n","                print(f\"Processing video: {video_file}\")\n","\n","                # Process the video\n","                face_matches =  process_video_and_search(video_file, emb_dict, output_dir, save_n = save_n)\n","\n","                processed_videos.append(os.path.abspath(video_file))\n","\n","          meta_checkpoint['processed_videos'] = processed_videos\n","          meta_checkpoint['long_videos'] = long_videos\n","          with open(meta_checkpoint_path, 'wb') as meta_file:\n","            pickle.dump(meta_checkpoint, meta_file)\n","          print(f\"Checkpoint saved after processing video: {video_file}\")\n","\n","          pbar.update(1)\n","\n","    print(\"All videos processed.\")\n","\n","# Example usage\n","input_folder = \"/content/drive/MyDrive/COPY/VOL00002\"\n","output_dir = \"/content/drive/MyDrive/faces/search_output/VOL22\"\n","run_directory(input_folder, filtered, output_dir, save_n=500)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[{"file_id":"1FDycZvDrq0sxCEfMn1YVJtuXcgsAZqiy","timestamp":1724257743545}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0206f067627d46f88d85952e43b6e17b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05e1d5f39d0f4a609a165b2b044fdbc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6531368ec5da4835899d15625aae01c3","IPY_MODEL_645cbe40071a4c31bb1b09971e75cbbe","IPY_MODEL_eb28a9116cda478eb55389dbf289122b"],"layout":"IPY_MODEL_83b43ac0d98840528a2599e5297c36b8"}},"074ddb51f9004ea79c66394da0196a11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eb1511489544cc9b6cc8b037317561b","max":560,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f6c0d85509542a7a899e4f397fef37d","value":560}},"13eaedaf13c240a3acd07bba5cdb23ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f30b1f8896b349af846e015d0b0ba160","placeholder":"​","style":"IPY_MODEL_29eee78998a7498c92a6872de2ffad2d","value":" 3.44G/3.44G [00:50&lt;00:00, 81.9MB/s]"}},"146b8abf6bd34493bbad696b0df3e202":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29eee78998a7498c92a6872de2ffad2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a670ac2de644862adb36beab06c4923":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2eb1511489544cc9b6cc8b037317561b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ed171c407264eef81cbfcaffefc8e97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3044309fc77b40feaa12d172fb1c4f6e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64f076f2003d4155b73c5a9aa8bfae40","IPY_MODEL_074ddb51f9004ea79c66394da0196a11","IPY_MODEL_4ce559e8ecf042fdb317d9f6b55851fc"],"layout":"IPY_MODEL_67aeab169b1d44dc8236af06a6533065"}},"39e31e4fe03c40569f4fa5d496eccec9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_574ff0a361e245f8a8407300bd1d1382","IPY_MODEL_e7b01e10ab1a48979e6f49b94f92e1eb","IPY_MODEL_ede265d81a574adc84baf10be2fdc1ca"],"layout":"IPY_MODEL_6a34160e018a480a82344a24a9cc9a2f"}},"43a8eaf15df04a94872fc19fa0517608":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"489f7118949046ae837bb7e8d01275c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ae2d051e3584d3c850ccacd510bccb4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ce559e8ecf042fdb317d9f6b55851fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8151cb09dfda4a30aaf92eaa2cf6c4aa","placeholder":"​","style":"IPY_MODEL_eb4daddf094e41d68e1f460e2c59f7b9","value":" 560/560 [00:00&lt;00:00, 43.6kB/s]"}},"53e43a5cbd4541cda769b09e5b210694":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0206f067627d46f88d85952e43b6e17b","max":1779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed64d4fedb194070b9521dd3c6129a2f","value":1779}},"53eefb632b7d450caa3c536ab84a2f3e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55b15109da4c4578923062a3d44527f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"574ff0a361e245f8a8407300bd1d1382":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_146b8abf6bd34493bbad696b0df3e202","placeholder":"​","style":"IPY_MODEL_6d1c5981d2894f35ac8f5d30ffe6161e","value":"pytorch_model.bin: 100%"}},"5f6c0d85509542a7a899e4f397fef37d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61cf0f3107e3489395efb95422a05fc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae8f34d87a58406ca5220d3aed01ec29","placeholder":"​","style":"IPY_MODEL_6518a7f83c564b819738852a588e8f74","value":"diffusion_pytorch_model.safetensors: 100%"}},"645cbe40071a4c31bb1b09971e75cbbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b891923404c44a43aebfd215d86e15d2","max":260694151,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43a8eaf15df04a94872fc19fa0517608","value":260694151}},"64f076f2003d4155b73c5a9aa8bfae40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6dca33c465cb49509078b93b81a77520","placeholder":"​","style":"IPY_MODEL_9ead95cbd9f5452799a155c40b7e8278","value":"encoder/config.json: 100%"}},"6518a7f83c564b819738852a588e8f74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6531368ec5da4835899d15625aae01c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84e447f30ca54c3cab4c1df1ae4f6f08","placeholder":"​","style":"IPY_MODEL_9b9af81379344a228c64839c406e5b95","value":"arcface.onnx: 100%"}},"67aeab169b1d44dc8236af06a6533065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a34160e018a480a82344a24a9cc9a2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d1c5981d2894f35ac8f5d30ffe6161e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6dca33c465cb49509078b93b81a77520":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"721b131188594c388a4f55cf6c1a4fb9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d5d08dcfc994e38b128edefc747d61f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_489f7118949046ae837bb7e8d01275c4","placeholder":"​","style":"IPY_MODEL_b4235b7bd79c429a87893bb654de76c4","value":"arc2face/config.json: 100%"}},"8151cb09dfda4a30aaf92eaa2cf6c4aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83b43ac0d98840528a2599e5297c36b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84e447f30ca54c3cab4c1df1ae4f6f08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"874ba4cc5bf643feb474bd5007fc13e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9102f1be02d64b4d96e82415dc239d96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91309492784843aa8a3a2543b8ab63bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"932bf349ce28475080ceea5a62ea87df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b9af81379344a228c64839c406e5b95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ead95cbd9f5452799a155c40b7e8278":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa0929f71d934b52b76497456b2d729e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"abc3537f0ef44211b7b3c236766fc499":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae8f34d87a58406ca5220d3aed01ec29":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0dcd71240c44a20baccfc45d5a1d33f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d5d08dcfc994e38b128edefc747d61f","IPY_MODEL_53e43a5cbd4541cda769b09e5b210694","IPY_MODEL_ee70a9f244d74ff5bc072189dfb5b453"],"layout":"IPY_MODEL_2ed171c407264eef81cbfcaffefc8e97"}},"b4235b7bd79c429a87893bb654de76c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b891923404c44a43aebfd215d86e15d2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c88f10a7cff8450fbf20ea82b4236850":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61cf0f3107e3489395efb95422a05fc2","IPY_MODEL_ee177a0daa57452dade440c5fa1d0ae2","IPY_MODEL_13eaedaf13c240a3acd07bba5cdb23ca"],"layout":"IPY_MODEL_4ae2d051e3584d3c850ccacd510bccb4"}},"e7b01e10ab1a48979e6f49b94f92e1eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_abc3537f0ef44211b7b3c236766fc499","max":492308829,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aa0929f71d934b52b76497456b2d729e","value":492308829}},"eb28a9116cda478eb55389dbf289122b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_721b131188594c388a4f55cf6c1a4fb9","placeholder":"​","style":"IPY_MODEL_9102f1be02d64b4d96e82415dc239d96","value":" 261M/261M [00:07&lt;00:00, 24.2MB/s]"}},"eb4daddf094e41d68e1f460e2c59f7b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed64d4fedb194070b9521dd3c6129a2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ede265d81a574adc84baf10be2fdc1ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53eefb632b7d450caa3c536ab84a2f3e","placeholder":"​","style":"IPY_MODEL_932bf349ce28475080ceea5a62ea87df","value":" 492M/492M [00:06&lt;00:00, 81.3MB/s]"}},"ee177a0daa57452dade440c5fa1d0ae2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_55b15109da4c4578923062a3d44527f8","max":3438167536,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a670ac2de644862adb36beab06c4923","value":3438167536}},"ee70a9f244d74ff5bc072189dfb5b453":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91309492784843aa8a3a2543b8ab63bd","placeholder":"​","style":"IPY_MODEL_874ba4cc5bf643feb474bd5007fc13e1","value":" 1.78k/1.78k [00:00&lt;00:00, 106kB/s]"}},"f30b1f8896b349af846e015d0b0ba160":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b609d21faaa34904b1ecbcc18ac971e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f0cec55c6c64066a18b724d2f0bf5cd","IPY_MODEL_12a9abc10d3240b5a55c177d6760ea2c","IPY_MODEL_263b7521663d4485aa8cf50d6bbad07a"],"layout":"IPY_MODEL_98a4d5b49a0347d0b6dc9cb4805f19e9"}},"2f0cec55c6c64066a18b724d2f0bf5cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ea09e7b8c334a9c9d4a4df5255d707f","placeholder":"​","style":"IPY_MODEL_9c8eb1403d814adab24cf845855a69e3","value":"Processing DEFVID_000000165 frame 155/5788:   3%"}},"12a9abc10d3240b5a55c177d6760ea2c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cbdbcf189074b749e0ab8e17a936351","max":5788,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0442f8e4d10947e68971939a593d35df","value":154}},"263b7521663d4485aa8cf50d6bbad07a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88278a9ecd5b4406bf8501f0deecec7f","placeholder":"​","style":"IPY_MODEL_b6e007a5afb7458181bc12bf28f3e8f8","value":" 154/5788 [38:04&lt;20:02:42, 12.81s/it]"}},"98a4d5b49a0347d0b6dc9cb4805f19e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ea09e7b8c334a9c9d4a4df5255d707f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c8eb1403d814adab24cf845855a69e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8cbdbcf189074b749e0ab8e17a936351":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0442f8e4d10947e68971939a593d35df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"88278a9ecd5b4406bf8501f0deecec7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6e007a5afb7458181bc12bf28f3e8f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}